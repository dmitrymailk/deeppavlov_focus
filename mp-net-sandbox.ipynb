{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  MPNetModel, MPNetForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from transformers.models.mpnet.modeling_mpnet import MPNetClassificationHead, SequenceClassifierOutput\n",
    "from typing import List, Optional, Union, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers.utils import ModelOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassificationV1 were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 197kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 390kB/s]  \n",
      "Downloading tokenizer.json: 100%|██████████| 455k/455k [00:00<00:00, 606kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 121kB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MPNetForSequenceClassificationV1(MPNetForSequenceClassification):\n",
    "    def __init__(self, \n",
    "            config,\n",
    "            cross_entropy_loss_weights=None\n",
    "        ):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.mpnet = MPNetModel(config)\n",
    "        self.classifier = MPNetClassificationHead(config)\n",
    "        self.cross_entropy_loss_weights = cross_entropy_loss_weights\n",
    "        \n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mpnet(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = None\n",
    "            if self.cross_entropy_loss_weights is None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss(\n",
    "                    weight=torch.tensor(self.cross_entropy_loss_weights, dtype=torch.float32)\n",
    "                )\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0100,  0.0156]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.batch_encode_plus([\"Hello world\"], return_tensors=\"pt\")\n",
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 493/493 [00:00<00:00, 925kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 513kB/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 461k/461k [00:00<00:00, 614kB/s]  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/mpnet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.model_max_length\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test sentence transformer valid - should be 94.5% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 2022\n",
      "Some weights of MPNetForSentenceEmbeddingV1 were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from core.base_models.mpnet_models import (\n",
    "    MPNetForSequenceClassificationV1,\n",
    "    MPNetForSequenceClassificationV2,\n",
    "    MPNetForSentenceEmbeddingV1,\n",
    ")\n",
    "from core.dataloaders.focus.focus_dataloader import (\n",
    "    FoCusDatasetKnowledgeV3,\n",
    "    FoCusDatasetKnowledgeV4,\n",
    "    FoCusDatasetPersonaV2,\n",
    ")\n",
    "from core.lighting_models.mpnet_lighting import MPNetKnowledgeLightningModelV1\n",
    "from core.dataloaders.focus.models.mpnet_dataloaders import (\n",
    "    MPNetFoCusPersonaDatasetSampleV1,\n",
    ")\n",
    "from core.hyperparameters.lighting_hyperparameters import LightingHyperparametersV1\n",
    "from core.hyperparameters.mpnet_hyperparameters import MPNetHyperparametersV1\n",
    "from core.loggers.wandb_logger import WandbLoggerV2\n",
    "from core.utils import (\n",
    "    ExperimentArgumentParserV1,\n",
    "    PytorchDatasetFactory,\n",
    "    TrainArgumentsV1,\n",
    ")\n",
    "\n",
    "from datasets import load_metric  # type: ignore\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers as tr\n",
    "\n",
    "\n",
    "from core.dataloaders.focus.lighting.mpnet_lighting_dataloader import (\n",
    "    MPNetLightingDataModuleV1,\n",
    ")\n",
    "from core.dataloaders.focus.models.mpnet_dataloaders import (\n",
    "    MPNetFoCusKnowledgeDatasetSampleV1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "max_epochs = 4\n",
    "max_epochs = 1\n",
    "\n",
    "lighting_hyperparameters = LightingHyperparametersV1(\n",
    "    precision=16,\n",
    "    max_epochs=max_epochs,\n",
    ").__dict__\n",
    "\n",
    "hyperparameters = MPNetHyperparametersV1(\n",
    "    lighting_hyperparameters=lighting_hyperparameters,\n",
    "    project_name=\"focus_knowledge_classification\",\n",
    ")\n",
    "seed_everything(hyperparameters.seed)\n",
    "\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(hyperparameters.model_name)  # type: ignore\n",
    "is_debug = 0\n",
    "\n",
    "data_module = MPNetLightingDataModuleV1(\n",
    "    train_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "    valid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    debug_status=is_debug,\n",
    "    base_train_dataset_class=FoCusDatasetKnowledgeV4,\n",
    "    base_valid_dataset_class=FoCusDatasetKnowledgeV3,\n",
    "    base_train_sample_class=MPNetFoCusKnowledgeDatasetSampleV1,\n",
    "    base_valid_sample_class=MPNetFoCusKnowledgeDatasetSampleV1,\n",
    ")\n",
    "\n",
    "base_model = MPNetForSentenceEmbeddingV1.from_pretrained(hyperparameters.model_name)\n",
    "\n",
    "model = MPNetKnowledgeLightningModelV1(\n",
    "    hyperparameters=hyperparameters,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    base_model=base_model,  # type: ignore\n",
    ")\n",
    "\n",
    "# accelerator = \"cpu\"\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "# ckpt_path = \"\"  # noqa: E501\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=accelerator,\n",
    "    **lighting_hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 14098/14098 [03:14<00:00, 72.30it/s]accuracy 0.9413016492285866\n",
      "Validation DataLoader 0: 100%|██████████| 14098/14098 [03:15<00:00, 72.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MPNetTokenizerFast' object has no attribute 'batch_encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tok \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/mpnet-base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m tok\u001b[39m.\u001b[39;49mbatch_encode([\u001b[39m\"\u001b[39m\u001b[39mHello world\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MPNetTokenizerFast' object has no attribute 'batch_encode'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # type: ignore\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/mpnet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 7596, 2092,    2,    1,    1,    1],\n",
       "        [   0, 2133, 2028, 2021, 2277, 1033,    2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.batch_encode_plus(\n",
    "    [\"Hello world\", \"How are you men?\"], \n",
    "    return_tensors=\"pt\", \n",
    "    truncation=True, \n",
    "    padding=\"longest\"\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.inference.inference_scripts import FocusKnowledgeKandidateExtractorDictV1\n",
    "from core.base_models.mpnet_models import MPNetForSentenceEmbeddingV1\n",
    "from transformers import AutoTokenizer # type: ignore\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "from typing import List\n",
    "\n",
    "class FocusKnowledgeKandidateExtractorV2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"all-mpnet-base-v2\",\n",
    "        tokenizer_name: str = \"all-mpnet-base-v2\",\n",
    "    ) -> None:\n",
    "        self.model_name = model_name\n",
    "        self.model = MPNetForSentenceEmbeddingV1.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)  # type: ignore\n",
    "        self.model.eval() # type: ignore\n",
    "\n",
    "    def extract(\n",
    "        self,\n",
    "        persona: List[str],\n",
    "        query: str,\n",
    "        knowledge_candidates: List[str],\n",
    "    ) -> FocusKnowledgeKandidateExtractorDictV1:\n",
    "        _persona = \" \".join(persona)\n",
    "        query = query + \" \" + _persona\n",
    "\n",
    "        encoded_query = self.tokenizer.batch_encode_plus(\n",
    "            [query],\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        encoded_knowledge_candidates = self.tokenizer.batch_encode_plus(\n",
    "            knowledge_candidates,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        encoded_query = self.model( # type: ignore\n",
    "            **encoded_query,\n",
    "        )\n",
    "\n",
    "        encoded_knowledge_candidates = self.model( # type: ignore\n",
    "            **encoded_knowledge_candidates,\n",
    "        )\n",
    "\n",
    "        cosine_scores = util.cos_sim(encoded_knowledge_candidates, encoded_query)  # type: ignore\n",
    "        print(cosine_scores)\n",
    "        top_indices = cosine_scores.topk(1, dim=0).indices.flatten().tolist()\n",
    "        top_sentences = [knowledge_candidates[i] for i in top_indices]\n",
    "        return FocusKnowledgeKandidateExtractorDictV1(\n",
    "            predicted_index=top_indices[0],\n",
    "            predicted_knowledge=top_sentences[0],\n",
    "        )\n",
    "        \n",
    "        \n",
    "extractor = FocusKnowledgeKandidateExtractorV2(\n",
    "    model_name=\"/home/dimweb/Desktop/deeppavlov/my_focus/models/knowledge-all-mpnet-base-v2-epoch=02-valid_accuracy=0.99\",\n",
    "    tokenizer_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2927],\n",
      "        [0.7791],\n",
      "        [0.3883]], device='cuda:0', grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predicted_index': 1,\n",
       " 'predicted_knowledge': 'Paris is the capital of France.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.extract(\n",
    "    persona=[\"I am a student\"],\n",
    "    query=\"What is the capital of France?\",\n",
    "    knowledge_candidates=[\n",
    "        \"London is the capital of England.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"Berlin is the capital of Germany.\",\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
