{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from transformers import AutoTokenizer, BartTokenizer\n",
    "\n",
    "class FoCusDatasetSampleV1:\n",
    "\t# вероятно могут быть проблемы с кодировкой во всех полях\n",
    "\t__slots__ = (\n",
    "\t\t'persona', \n",
    "\t\t'knowledge_candidates',  \n",
    "\t\t'persona_grounding', \n",
    "\t\t'dialog', \n",
    "\t\t'knowledge_answer_index',\n",
    "\t)\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\tpersona: List[str],\n",
    "\t\t\tknowledge_candidates: List[str],\n",
    "\t\t\tpersona_grounding: List[int],\n",
    "\t\t\tdialog: List[str],\n",
    "\t\t\tknowledge_answer_index: int,\n",
    "\t\t) -> None:\n",
    "\t\tself.persona = persona\n",
    "\t\tself.knowledge_candidates = knowledge_candidates\n",
    "\t\tself.persona_grounding = persona_grounding\n",
    "\t\tself.knowledge_answer_index = knowledge_answer_index\n",
    "\t\tself.dialog = dialog\n",
    "\t\n",
    "\tdef get_dict(self) -> dict:\n",
    "\t\treturn {\n",
    "\t\t\t'persona': self.persona,\n",
    "\t\t\t'knowledge_candidates': self.knowledge_candidates,\n",
    "\t\t\t'persona_grounding': self.persona_grounding,\n",
    "\t\t\t'dialog': self.dialog,\n",
    "\t\t\t'knowledge_answer_index': self.knowledge_answer_index,\n",
    "\t\t}\n",
    "\n",
    "class FoCusDatasetV1:\n",
    "\tdef __init__(self,\n",
    "\t\tinput_dataset_path: str = None,\n",
    "\t\t) -> None:\n",
    "\t\tassert input_dataset_path is not None, 'input_dataset_path is None'\n",
    "\n",
    "\t\tself.input_dataset_path = input_dataset_path\n",
    "\t\tself.dataset: List[FoCusDatasetSampleV1] = []\n",
    "\n",
    "\t\tself.__build_dataset()\n",
    "\t\n",
    "\tdef __build_dataset(self) -> None:\n",
    "\t\tinitial_train_dataset = self.__read_dataset(self.input_dataset_path)\n",
    "\t\tself.dataset = self.__create_initial_dataset(initial_train_dataset)\n",
    "\t\n",
    "\tdef __create_initial_dataset(self, initial_dataset: Dict) -> List[FoCusDatasetSampleV1]:\n",
    "\t\tdataset = []\n",
    "\t\tinitial_dataset_data = initial_dataset['data']\n",
    "\t\t\n",
    "\t\tfor i, dialog_set in enumerate(initial_dataset_data):\n",
    "\t\t\tpersona = dialog_set['persona']\n",
    "\t\t\tutterances = dialog_set['utterance']\n",
    "\t\t\t\n",
    "\t\t\tfor j, utterance in enumerate(utterances):\n",
    "\t\t\t\tpersona_grounding = list(map(int, utterance['persona_grounding']))\n",
    "\t\t\t\tknowledge_candidates = utterance['knowledge_candidates']\n",
    "\t\t\t\tknowledge_answer_index = utterance['knowledge_answer_index']\n",
    "\t\t\t\tdialog_index_key = [item for item in utterance.keys() if 'dialog' in item][0]\n",
    "\t\t\t\tdialog = utterance[dialog_index_key]\n",
    "\t\t\t\t\n",
    "\t\t\t\tdata_sample = FoCusDatasetSampleV1(\n",
    "\t\t\t\t\tpersona=persona,\n",
    "\t\t\t\t\tknowledge_candidates=knowledge_candidates,\n",
    "\t\t\t\t\tpersona_grounding=persona_grounding,\n",
    "\t\t\t\t\tdialog=dialog,\n",
    "\t\t\t\t\tknowledge_answer_index=knowledge_answer_index,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tdata_sample = data_sample.get_dict()\n",
    "\t\t\t\tdataset.append(data_sample)\n",
    "\t\t\n",
    "\t\treturn dataset\n",
    "\t\n",
    "\tdef __read_dataset(self, input_path: str) -> list:\n",
    "\t\twith open(input_path, 'r') as f:\n",
    "\t\t\tdataset = json.load(f)\n",
    "\t\treturn dataset\n",
    "\n",
    "class BartFoCusDatasetSampleHyperparameters:\n",
    "\tdef __init__(self,\n",
    "\t\t\thistory_length: int = 1,\n",
    "\t\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\thistory_length (int): количество пар диалогов(назад), которые будут использоваться для генерации ответа\t\n",
    "\t\t\"\"\"\n",
    "\t\tself.history_length = history_length\n",
    "\n",
    "class BartFoCusDatasetSampleV1:\n",
    "\t\"\"\"\n",
    "\tв этом датасете будет просто языковое моделирование\n",
    "\tс вставкой информацией о персоне и базы знаний + сами диалоги.\n",
    "\t- предложение из персоны будет вставляться только то что использовалось для генерации ответа\n",
    "\t- с предложением из базы знаний аналогично\n",
    "\t- knowledge_candidates это предложения из базы знаний отобранные при помощи tf-idf(похожие на вопрос пользователя) \n",
    "\t\t+ 1 предложение которое точно использовалось для ответа. при инференсе мы будем подавать знания просто отобранные\n",
    "\t\tпри помощи tf-idf \n",
    "\tпримерно так:\n",
    "\t\t[BOS] [persona] [SEP] [knowledge_candidates] [SEP] [dialog] [SEP] \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\tfocus_dataset_sample: FoCusDatasetSampleV1,\n",
    "\t\t\ttokenizer: BartTokenizer,\n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparameters,\n",
    "\t\t) -> None:\n",
    "\t\tself.focus_dataset_sample = focus_dataset_sample\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\n",
    "\t\tself.bos_token = self.tokenizer.bos_token\n",
    "\t\tself.pad_token = self.tokenizer.pad_token\n",
    "\t\tself.unk_token = self.tokenizer.unk_token\n",
    "\t\tself.sep_token = self.tokenizer.sep_token\n",
    "\t\tself.cls_token = self.tokenizer.cls_token\n",
    "\t\t\n",
    "\n",
    "\tdef get_dict(self) -> dict:\n",
    "\t\t...\n",
    "\n",
    "class PytorchFoCusDatasetV1(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tdataset: FoCusDatasetV1,\n",
    "\t\t) -> None:\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\t\tself.bart_hyperparameters = BartFoCusDatasetSampleHyperparameters()\n",
    "\t\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleV1:\n",
    "\t\tdataset_sample: FoCusDatasetSampleV1 = self.dataset[index]\n",
    "\t\ttrain_sample: BartFoCusDatasetSampleV1 = BartFoCusDatasetSampleV1(\n",
    "\t\t\tfocus_dataset_sample=dataset_sample,\n",
    "\t\t\ttokenizer=self.bart_tokenizer,\n",
    "\t\t\thyperparameters=self.bart_hyperparameters,\n",
    "\t\t)\n",
    "\t\treturn train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'persona': [\n",
    "\t'I would like to visit the Nazareth House again.',\n",
    "\t'I love Benevolent institutions.',\n",
    "\t'I am interested in History.',\n",
    "\t'I have curiosity about the Description of this place.',\n",
    "\t'I would like to know when it was Built.'\n",
    "],\n",
    " 'knowledge_candidates': [\n",
    "\t'Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia.',\n",
    "  \t'However, in many cases, a hearing is not held.',\n",
    "  \t'The church and school buildings are listed together as a Cleveland Designated Landmark.',\n",
    "  \t\"Until the reorganisation of London's local government in 1965, Muswell Hill formed part of the Borough of Hornsey within the administrative county of Middlesex.\",\n",
    "  \t'This operation enabled the Canadian Sulpicians to expand their primary work, the education of priests.',\n",
    "  \t\"Bosworth's design was heavily Greek-influenced: though the facade is made of white Vermont granite, it features layers of gray granite columns in Doric and Ionic styles, as well as various Greek-inspired ornamentation.\",\n",
    "  \t'The Insurance Hall is designated as a Grade II listed building, in part due to these murals.',\n",
    "  \t'It has been pointed out that this need could have been met with the man-made Stagnum (lake) of Agrippa or, more likely, the Euripus (canal) which allowed for runoff from the Stagnum to flow into the Tiber (please see below for more information on both the Stagnum and the Euripus).',\n",
    "  \t'By 1217, documents show that the castle at Almeida is one of several strong points that guard the border between Spain and Portugal.',\n",
    "  \t'The Riverwalk runs along much of the Brisbane River foreshore throughout the inner-city area, with the longest span running between Newstead and Toowong.'],\n",
    " 'persona_grounding': [1, 0, 0, 0, 0],\n",
    " 'dialog': [\n",
    "\t\"I think I've been there before but I don't remember the name of this place.\",\n",
    "  \t'This place is the Nazareth House, which you would like to visit again.'],\n",
    " 'knowledge_answer_index': 0\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = FoCusDatasetV1(input_dataset_path='./datasets/FoCus/valid_focus.json')\n",
    "temp = temp.dataset[1]\n",
    "\n",
    "history_length = 1\n",
    "context_length = 1\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "# ids = tokenizer.encode('hello worldish', add_special_tokens=False)\n",
    "encoded_persona = tokenizer.batch_encode_plus(temp['persona'], add_special_tokens=False)\n",
    "\n",
    "dialog_history = temp['dialog'][-2*history_length:]\n",
    "dialog_history_feature = tokenizer.batch_encode_plus(dialog_history[:-1], add_special_tokens=False)\n",
    "dialog_history_target = tokenizer.batch_encode_plus(dialog_history[-1:], add_special_tokens=False)\n",
    "\n",
    "true_knowledge_answer = [ temp['knowledge_candidates'][temp['knowledge_answer_index']] ]\n",
    "true_knowledge_answer = tokenizer.batch_encode_plus(true_knowledge_answer, add_special_tokens=False)\n",
    "\n",
    "knowledge_candidates = temp['knowledge_candidates']\n",
    "knowledge_candidates = tokenizer.batch_encode_plus(knowledge_candidates, add_special_tokens=False)\n",
    "\n",
    "context_knowledge_candidates = dialog_history_feature['input_ids'][-context_length:]\n",
    "context_knowledge_candidates, knowledge_candidates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
