{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetV1\n",
    "\n",
    "dataset = FoCusDatasetV1(\n",
    "    input_dataset_path=\"./datasets/FoCus/train_focus.json\"\n",
    ")\n",
    "\n",
    "valid_dataset = FoCusDatasetV1(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.utils import TfIdf, FoCusTfIdf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standart sklearn TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar(corpus, X, query, top_k=1):\n",
    "    similarity = cosine_similarity(X, query)\n",
    "    similarity = similarity.flatten()\n",
    "    similarity = np.argsort(similarity)[::-1][:top_k]\n",
    "    similarity = similarity.tolist()\n",
    "\n",
    "    similar_samples = [corpus[i] for i in similarity]\n",
    "    return similar_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.4497383836660411\n",
      "Top 3: 0.6737758061764204\n",
      "Top 5: 0.7954416197463459\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test', 'sentence']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Test sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nick likes to play football however he is not too fond of tennis'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "def clean_sentence(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    # words = [word for word in words if not word in all_stopwords]\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "clean_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.4494255815276119\n",
      "Top 3: 0.6742450093840642\n",
      "Top 5: 0.7950435079337997\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    \n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    # top_10 = top_similar(corpus, X, query_vector, top_k=10)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(true_candidate)\n",
    "    # pprint(corpus)\n",
    "    # pprint(true_candidate in top_10)\n",
    "    # print(top_10)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query with persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6005374509469373\n",
      "Top 3: 0.8148922254450321\n",
      "Top 5: 0.9023346414150031\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6079092037595318\n",
      "Top 3: 0.8251463025359106\n",
      "Top 5: 0.9109771236034758\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6123426139386416\n",
      "Top 3: 0.8233729384642667\n",
      "Top 5: 0.9092037595318319\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.5998976283910595\n",
      "Top 3: 0.8158732866973781\n",
      "Top 5: 0.901936529602457\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with transformer tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "from core.utils import FoCusTfIdf\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "original_texts = [\n",
    "    \"The fort was built in response to the growth of overland emigration to Oregon after 1845. The fort was built in response to the growth of overland emigration to Oregon after 1845. Livestock could be traded for fresh stock and letters sent back to the States.\",\n",
    "    \"The Truman Galusha House, also called the Truman Galusha Mansion and \\\"Fairview\\\" in various historical documents and maps, is a Federal-style house in Jericho, Vermont, United States. It was listed on the National Register of Historic Places in 1978 as the Galusha House, qualifying for designation based on its \\\"architectural excellence\\\" and the association of its early owners with important early governors and other key leaders involved with the creation of the state of Vermont. It was built in 1790, and is named for the son of an early Vermont governor, Jonas Galusha.\",\n",
    "    \"The Machzike Hadath Synagogue moved to Golders Green in the 1970s, opening its present building in 1983. A significant moment in Temple Fortune's development into a suburban area occurred in 1907, when transport links were vastly improved by the opening of Golders Green Underground station. [citation needed] Golders Green is home to a growing Japanese and East Asian community with many families living in the district being catered for a notable number of restaurants and shops specialising in Japanese and other East Asian food, such as the Seoul Plaza supermarket.\",\n",
    "    \"An alternative interpretation of the three-bodied goddess in Gangadharamurti panel here and elsewhere is that it represents the regenerative powers of rivers in the form of Mandakini, Suradhani and Bhagavati. The relief is much ruined below the waist, is 3.5\\u00a0m (11\\u00a0ft) high and posed in action. On the east side of the main hall is a separate shrine.\",\n",
    "    \"With the market for expensive luxury cars severely undercut by the Great Depression, Duesenberg folded in 1937. It was so reputed and imposing that many Hollywood stars, such as James Cagney, posed next to the car to promote their careers. It was also both the fastest and most expensive American automobile in the market.\",\n",
    "]\n",
    "query = \"Wow, this is amazing! What is this?\"\n",
    "corpus = tokenizer.batch_encode_plus(\n",
    "    original_texts, \n",
    "    padding=False, \n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "query = tokenizer.batch_encode_plus(\n",
    "    [query], \n",
    "    padding=False, \n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "tf_idf = FoCusTfIdf(corpus=corpus['input_ids'])\n",
    "\n",
    "tf_idf.top_similar(\n",
    "    query=query['input_ids'], \n",
    "    top_k=1,\n",
    "    return_indices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.5114381982621032\n",
      "Top 3: 0.5114381982621032\n",
      "Top 5: 0.5114381982621032\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    encoded_query = tokenizer.batch_encode_plus(\n",
    "        [query],\n",
    "        padding=False, \n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    encoded_corpus = tokenizer.batch_encode_plus(\n",
    "        corpus,\n",
    "        padding=False, \n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "        # max_length=100,\n",
    "    )\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    tf_idf = FoCusTfIdf(corpus=encoded_corpus['input_ids'])\n",
    "\n",
    "    top_1 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=1,return_indices=True)\n",
    "    # print(top_1)\n",
    "    top_1 = [corpus[i] for i in top_1]\n",
    "    top_3 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=3,return_indices=True)\n",
    "    top_3 = [corpus[i] for i in top_3]\n",
    "    top_5 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=5,return_indices=True)\n",
    "    top_5 = [corpus[i] for i in top_5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0543],\n",
       "        [0.2277],\n",
       "        [0.2838]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    'The new movie is awesome',\n",
    "    'A man is playing guitar',\n",
    "    'The cat sits outside',\n",
    "]\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',]\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "cosine_scores\n",
    "# #Output the pairs with their score\n",
    "# for i in range(len(sentences1)):\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.2838],\n",
       "        [0.2277]], device='cuda:0'),\n",
       "indices=tensor([[2],\n",
       "        [1]], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores.topk(2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar_sentences(X, query, top_k=1):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9343855293491754\n",
      "Top 3: 0.9863450966483419\n",
      "Top 5: 0.9964532718567122\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9348518455326167\n",
      "Top 3: 0.9883978843200819\n",
      "Top 5: 0.9964454302451231\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6586274162085476\n",
      "Top 3: 0.8315304131938287\n",
      "Top 5: 0.9207306259975173\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9347402021635042\n",
      "Top 2: 0.9746408937754921\n",
      "Top 3: 0.9863450966483419\n",
      "Top 5: 0.9962759354495478\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_2 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_2 = top_sentences[:2]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_2:\n",
    "        correct_top_2 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 2: {correct_top_2 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 385kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 50.9kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.84MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 170kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 35.0kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 263kB/s] \n",
      "Downloading: 100%|██████████| 438M/438M [01:26<00:00, 5.08MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 16.7kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 76.3kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 805kB/s] \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 94.3kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 3.66MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 400kB/s]  \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 115kB/s]\n"
     ]
    }
   ],
   "source": [
    "best_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9413016492285866\n",
      "Top 2: 0.9785422947331087\n",
      "Top 3: 0.9907785068274517\n",
      "Top 5: 0.9975172902996985\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_2 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = best_model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = best_model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_2 = top_sentences[:2]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_2:\n",
    "        correct_top_2 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 2: {correct_top_2 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persona classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona accuracy: 0.8669976946267617\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "persona_correct = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona_grounding = item[\"persona_grounding\"]\n",
    "    true_persona = [pers for pers, ground in zip(persona, persona_grounding) if ground == 1]\n",
    "    # persona = \" \".join(persona)\n",
    "    # query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    query = query + \" \" + true_candidate\n",
    "\n",
    "    # query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    # corpus_emb = model.encode(persona, convert_to_tensor=True)\n",
    "\n",
    "    # cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "\n",
    "    # print( == [0, 0, 0, 1, 0])\n",
    "    # print(persona_grounding)\n",
    "    # predicts = (cosine_scores > 0.9).flatten().int()\n",
    "    predicts = torch.tensor([0, 0, 0, 0, 0]).to(predicts.device)\n",
    "    persona_grounding = torch.tensor(persona_grounding).to(predicts.device)\n",
    "    local_acc = (predicts == persona_grounding).sum().item() / 5\n",
    "    # if predicts == persona_grounding:\n",
    "    #     persona_correct += 1\n",
    "    persona_correct += local_acc\n",
    "    \n",
    "    # break\n",
    "\n",
    "print(f\"Persona accuracy: {persona_correct / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona accuracy: 0.8654581129506077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "persona_correct = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona_grounding = item[\"persona_grounding\"]\n",
    "    true_persona = [pers for pers, ground in zip(persona, persona_grounding) if ground == 1]\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    query = query + \" \" + true_candidate\n",
    "    predicts = torch.tensor([0, 0, 0, 0, 0]).to(predicts.device)\n",
    "    persona_grounding = torch.tensor(persona_grounding).to(predicts.device)\n",
    "    local_acc = (predicts == persona_grounding).sum().item() / 5\n",
    "    persona_correct += local_acc\n",
    "\n",
    "print(f\"Persona accuracy: {persona_correct / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore knowledge dataleak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge accuracy:  0.7226458591948927\n"
     ]
    }
   ],
   "source": [
    "knowledge_correct = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    knowledge_candidates = item[\"knowledge_candidates\"]\n",
    "    knowledge_index = item[\"knowledge_answer_index\"]\n",
    "    knowledge = item[\"knowledge\"]\n",
    "    predict_index = 0\n",
    "    stop = False\n",
    "\n",
    "    for i, candidate in enumerate(knowledge_candidates):\n",
    "        for knowledge_item in knowledge:\n",
    "            if candidate in knowledge_item:\n",
    "                predict_index = i\n",
    "                stop = True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "    if predict_index == knowledge_index:\n",
    "        knowledge_correct += 1\n",
    "  \n",
    "    # break\n",
    "print(\"Knowledge accuracy: \", knowledge_correct / len(valid_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
