{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetV1\n",
    "\n",
    "dataset = FoCusDatasetV1(\n",
    "    input_dataset_path=\"./datasets/FoCus/train_focus.json\"\n",
    ")\n",
    "\n",
    "valid_dataset = FoCusDatasetV1(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from core.utils import TfIdf, FoCusTfIdf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standart sklearn TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar(corpus, X, query, top_k=1):\n",
    "    similarity = cosine_similarity(X, query)\n",
    "    similarity = similarity.flatten()\n",
    "    similarity = np.argsort(similarity)[::-1][:top_k]\n",
    "    similarity = similarity.tolist()\n",
    "\n",
    "    similar_samples = [corpus[i] for i in similarity]\n",
    "    return similar_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.4497383836660411\n",
      "Top 3: 0.6737758061764204\n",
      "Top 5: 0.7954416197463459\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test', 'sentence']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Test sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nick likes to play football however he is not too fond of tennis'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "all_stopwords = nlp.Defaults.stop_words\n",
    "def clean_sentence(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    # words = [word for word in words if not word in all_stopwords]\n",
    "    sentence = \" \".join(words)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "\n",
    "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
    "clean_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.4494255815276119\n",
      "Top 3: 0.6742450093840642\n",
      "Top 5: 0.7950435079337997\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    \n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    # top_10 = top_similar(corpus, X, query_vector, top_k=10)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(true_candidate)\n",
    "    # pprint(corpus)\n",
    "    # pprint(true_candidate in top_10)\n",
    "    # print(top_10)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query with persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6005374509469373\n",
      "Top 3: 0.8148922254450321\n",
      "Top 5: 0.9023346414150031\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6079092037595318\n",
      "Top 3: 0.8251463025359106\n",
      "Top 5: 0.9109771236034758\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6123426139386416\n",
      "Top 3: 0.8233729384642667\n",
      "Top 5: 0.9092037595318319\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.5998976283910595\n",
      "Top 3: 0.8158732866973781\n",
      "Top 5: 0.901936529602457\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    query = clean_sentence(query)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    corpus = [clean_sentence(sent) for sent in corpus]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    top_1 = top_similar(corpus, X, query_vector, top_k=1)\n",
    "    top_3 = top_similar(corpus, X, query_vector, top_k=3)\n",
    "    top_5 = top_similar(corpus, X, query_vector, top_k=5)\n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with transformer tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "from core.utils import FoCusTfIdf\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "original_texts = [\n",
    "    \"The fort was built in response to the growth of overland emigration to Oregon after 1845. The fort was built in response to the growth of overland emigration to Oregon after 1845. Livestock could be traded for fresh stock and letters sent back to the States.\",\n",
    "    \"The Truman Galusha House, also called the Truman Galusha Mansion and \\\"Fairview\\\" in various historical documents and maps, is a Federal-style house in Jericho, Vermont, United States. It was listed on the National Register of Historic Places in 1978 as the Galusha House, qualifying for designation based on its \\\"architectural excellence\\\" and the association of its early owners with important early governors and other key leaders involved with the creation of the state of Vermont. It was built in 1790, and is named for the son of an early Vermont governor, Jonas Galusha.\",\n",
    "    \"The Machzike Hadath Synagogue moved to Golders Green in the 1970s, opening its present building in 1983. A significant moment in Temple Fortune's development into a suburban area occurred in 1907, when transport links were vastly improved by the opening of Golders Green Underground station. [citation needed] Golders Green is home to a growing Japanese and East Asian community with many families living in the district being catered for a notable number of restaurants and shops specialising in Japanese and other East Asian food, such as the Seoul Plaza supermarket.\",\n",
    "    \"An alternative interpretation of the three-bodied goddess in Gangadharamurti panel here and elsewhere is that it represents the regenerative powers of rivers in the form of Mandakini, Suradhani and Bhagavati. The relief is much ruined below the waist, is 3.5\\u00a0m (11\\u00a0ft) high and posed in action. On the east side of the main hall is a separate shrine.\",\n",
    "    \"With the market for expensive luxury cars severely undercut by the Great Depression, Duesenberg folded in 1937. It was so reputed and imposing that many Hollywood stars, such as James Cagney, posed next to the car to promote their careers. It was also both the fastest and most expensive American automobile in the market.\",\n",
    "]\n",
    "query = \"Wow, this is amazing! What is this?\"\n",
    "corpus = tokenizer.batch_encode_plus(\n",
    "    original_texts, \n",
    "    padding=False, \n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "query = tokenizer.batch_encode_plus(\n",
    "    [query], \n",
    "    padding=False, \n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "\n",
    "tf_idf = FoCusTfIdf(corpus=corpus['input_ids'])\n",
    "\n",
    "tf_idf.top_similar(\n",
    "    query=query['input_ids'], \n",
    "    top_k=1,\n",
    "    return_indices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.5114381982621032\n",
      "Top 3: 0.5114381982621032\n",
      "Top 5: 0.5114381982621032\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "    encoded_query = tokenizer.batch_encode_plus(\n",
    "        [query],\n",
    "        padding=False, \n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    encoded_corpus = tokenizer.batch_encode_plus(\n",
    "        corpus,\n",
    "        padding=False, \n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "        # max_length=100,\n",
    "    )\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    # print(true_candidate)\n",
    "    tf_idf = FoCusTfIdf(corpus=encoded_corpus['input_ids'])\n",
    "\n",
    "    top_1 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=1,return_indices=True)\n",
    "    # print(top_1)\n",
    "    top_1 = [corpus[i] for i in top_1]\n",
    "    top_3 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=3,return_indices=True)\n",
    "    top_3 = [corpus[i] for i in top_3]\n",
    "    top_5 = tf_idf.top_similar(query=encoded_query['input_ids'], top_k=5,return_indices=True)\n",
    "    top_5 = [corpus[i] for i in top_5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # print(persona)\n",
    "    # print(query)\n",
    "    break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0543],\n",
       "        [0.2277],\n",
       "        [0.2838]], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\n",
    "    'The new movie is awesome',\n",
    "    'A man is playing guitar',\n",
    "    'The cat sits outside',\n",
    "]\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',]\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "cosine_scores\n",
    "# #Output the pairs with their score\n",
    "# for i in range(len(sentences1)):\n",
    "#     print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.2838],\n",
       "        [0.2277]], device='cuda:0'),\n",
       "indices=tensor([[2],\n",
       "        [1]], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_scores.topk(2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar_sentences(X, query, top_k=1):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9343855293491754\n",
      "Top 3: 0.9863450966483419\n",
      "Top 5: 0.9964532718567122\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9348518455326167\n",
      "Top 3: 0.9883978843200819\n",
      "Top 5: 0.9964454302451231\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = persona + \" \" + query\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.6586274162085476\n",
      "Top 3: 0.8315304131938287\n",
      "Top 5: 0.9207306259975173\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9347402021635042\n",
      "Top 2: 0.9746408937754921\n",
      "Top 3: 0.9863450966483419\n",
      "Top 5: 0.9962759354495478\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_2 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_2 = top_sentences[:2]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_2:\n",
    "        correct_top_2 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 2: {correct_top_2 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 385kB/s]\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 50.9kB/s]\n",
      "Downloading: 100%|██████████| 10.6k/10.6k [00:00<00:00, 2.84MB/s]\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 170kB/s]\n",
      "Downloading: 100%|██████████| 116/116 [00:00<00:00, 35.0kB/s]\n",
      "Downloading: 100%|██████████| 39.3k/39.3k [00:00<00:00, 263kB/s] \n",
      "Downloading: 100%|██████████| 438M/438M [01:26<00:00, 5.08MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 16.7kB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 76.3kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 805kB/s] \n",
      "Downloading: 100%|██████████| 363/363 [00:00<00:00, 94.3kB/s]\n",
      "Downloading: 100%|██████████| 13.1k/13.1k [00:00<00:00, 3.66MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 400kB/s]  \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 115kB/s]\n"
     ]
    }
   ],
   "source": [
    "best_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1: 0.9413016492285866\n",
      "Top 2: 0.9785422947331087\n",
      "Top 3: 0.9907785068274517\n",
      "Top 5: 0.9975172902996985\n"
     ]
    }
   ],
   "source": [
    "correct_top_1 = 0\n",
    "correct_top_2 = 0\n",
    "correct_top_3 = 0\n",
    "correct_top_5 = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona = \" \".join(persona)\n",
    "    query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "\n",
    "    query_emb = best_model.encode([query], convert_to_tensor=True)\n",
    "    corpus_emb = best_model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "    top_indices = cosine_scores.topk(5, dim=0).indices.flatten().tolist()\n",
    "    # print(top_indices)\n",
    "    top_sentences = [corpus[i] for i in top_indices]\n",
    "    top_1 = top_sentences[:1]\n",
    "    top_3 = top_sentences[:3]\n",
    "    top_2 = top_sentences[:2]\n",
    "    top_5 = top_sentences[:5]\n",
    "    \n",
    "    if true_candidate in top_1:\n",
    "        correct_top_1 += 1\n",
    "    if true_candidate in top_2:\n",
    "        correct_top_2 += 1\n",
    "    if true_candidate in top_3:\n",
    "        correct_top_3 += 1\n",
    "    if true_candidate in top_5:\n",
    "        correct_top_5 += 1\n",
    "    # break\n",
    "\n",
    "print(f\"Top 1: {correct_top_1 / len(valid_dataset)}\")\n",
    "print(f\"Top 2: {correct_top_2 / len(valid_dataset)}\")\n",
    "print(f\"Top 3: {correct_top_3 / len(valid_dataset)}\")\n",
    "print(f\"Top 5: {correct_top_5 / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persona classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona accuracy: 0.8669976946267617\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "persona_correct = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona_grounding = item[\"persona_grounding\"]\n",
    "    true_persona = [pers for pers, ground in zip(persona, persona_grounding) if ground == 1]\n",
    "    # persona = \" \".join(persona)\n",
    "    # query = query + \" \" + persona\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    query = query + \" \" + true_candidate\n",
    "\n",
    "    # query_emb = model.encode([query], convert_to_tensor=True)\n",
    "    # corpus_emb = model.encode(persona, convert_to_tensor=True)\n",
    "\n",
    "    # cosine_scores = util.cos_sim(corpus_emb, query_emb)\n",
    "\n",
    "    # print( == [0, 0, 0, 1, 0])\n",
    "    # print(persona_grounding)\n",
    "    # predicts = (cosine_scores > 0.9).flatten().int()\n",
    "    predicts = torch.tensor([0, 0, 0, 0, 0]).to(predicts.device)\n",
    "    persona_grounding = torch.tensor(persona_grounding).to(predicts.device)\n",
    "    local_acc = (predicts == persona_grounding).sum().item() / 5\n",
    "    # if predicts == persona_grounding:\n",
    "    #     persona_correct += 1\n",
    "    persona_correct += local_acc\n",
    "    \n",
    "    # break\n",
    "\n",
    "print(f\"Persona accuracy: {persona_correct / len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona accuracy: 0.8654581129506077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "persona_correct = 0\n",
    "\n",
    "for item in dataset:\n",
    "    query = item[\"dialog\"][-2]\n",
    "    persona = item[\"persona\"]\n",
    "    persona_grounding = item[\"persona_grounding\"]\n",
    "    true_persona = [pers for pers, ground in zip(persona, persona_grounding) if ground == 1]\n",
    "\n",
    "    corpus = item[\"knowledge_candidates\"]\n",
    "    candidate_index = item[\"knowledge_answer_index\"]\n",
    "    true_candidate = corpus[candidate_index]\n",
    "    query = query + \" \" + true_candidate\n",
    "    predicts = torch.tensor([0, 0, 0, 0, 0]).to(predicts.device)\n",
    "    persona_grounding = torch.tensor(persona_grounding).to(predicts.device)\n",
    "    local_acc = (predicts == persona_grounding).sum().item() / 5\n",
    "    persona_correct += local_acc\n",
    "\n",
    "print(f\"Persona accuracy: {persona_correct / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore knowledge dataleak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'persona': ['I would like to visit the Nazareth House again.',\n",
       "  'I love Benevolent institutions.',\n",
       "  'I am interested in History.',\n",
       "  'I have curiosity about the Description of this place.',\n",
       "  'I would like to know when it was Built.'],\n",
       " 'knowledge_candidates': ['Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia.',\n",
       "  'However, in many cases, a hearing is not held.',\n",
       "  'The church and school buildings are listed together as a Cleveland Designated Landmark.',\n",
       "  \"Until the reorganisation of London's local government in 1965, Muswell Hill formed part of the Borough of Hornsey within the administrative county of Middlesex.\",\n",
       "  'This operation enabled the Canadian Sulpicians to expand their primary work, the education of priests.',\n",
       "  \"Bosworth's design was heavily Greek-influenced: though the facade is made of white Vermont granite, it features layers of gray granite columns in Doric and Ionic styles, as well as various Greek-inspired ornamentation.\",\n",
       "  'The Insurance Hall is designated as a Grade II listed building, in part due to these murals.',\n",
       "  'It has been pointed out that this need could have been met with the man-made Stagnum (lake) of Agrippa or, more likely, the Euripus (canal) which allowed for runoff from the Stagnum to flow into the Tiber (please see below for more information on both the Stagnum and the Euripus).',\n",
       "  'By 1217, documents show that the castle at Almeida is one of several strong points that guard the border between Spain and Portugal.',\n",
       "  'The Riverwalk runs along much of the Brisbane River foreshore throughout the inner-city area, with the longest span running between Newstead and Toowong.'],\n",
       " 'persona_grounding': [1, 0, 0, 0, 0],\n",
       " 'dialog': [\"I think I've been there before but I don't remember the name of this place.\",\n",
       "  'This place is the Nazareth House, which you would like to visit again.'],\n",
       " 'knowledge_answer_index': 0,\n",
       " 'knowledge': ['Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia. It was built from 1924 to 1939. It was added to the Queensland Heritage Register on 2 April 2002.',\n",
       "  'Nazareth House, situated on Tingal Hill, Wynnum, was designed by Brisbane architectural firm, Hennessy, Hennessy, Keesing & Co and JP Donoghue and built by George Turner. Nazareth House was officially opened by Archbishop James Duhig in 1925 as part of the charitable institution established on the site by the Poor Sisters of Nazareth.',\n",
       "  'A Catholic presence in Wynnum was prompted by the establishment of the area as a popular seaside resort. The opening of the Wynnum South railway station in 1898 encouraged further development, and the demand for a religious presence in the area grew. In 1903, the first Catholic masses were held in the Wynnum Shire Hall.',\n",
       "  'The Congregation of the Poor Sisters of Nazareth was founded in London in 1854 by Mother St Basil (born Victoire Larmenier on 21 July 1827 at Liffré, near Rennes, France). The order was established to care for the aged poor at the request of Cardinal Nicholas Wiseman, Archbishop of Westminster. The first foundation of the order to be established in Australia was at Ballarat in 1888. At the request of Archbishop Duhig, a Queensland chapter was founded in 1921. Archbishop Duhig had visited Nazareth House in London and subsequently petitioned the Mother-General to send a community of nuns to Queensland. Duhig then proceeded to search for a suitable site.',\n",
       "  \"In 1918, Duhig purchased Mt Margaret, a property of around 60 acres (24\\xa0ha) at Tingal Hill, Wynnum, for the purposes of establishing an aged persons and children's home to be run by the Sisters of Nazareth. The property, which included a large villa, was purchased for £5000 from the trustees of the estate of the late William Kidston, Premier of Queensland from 1906-07. Kidston had entered into an agreement with Duhig prior to his death for the sale of his property for the purposes of establishing a home for the needy.\",\n",
       "  'Due to the difficulties of transport after World War I, it was not possible to secure passages for the sisters until 15 January 1921. On this date, six women, Sister Francis Borgia, Sister St David, Sister M Fulgentia, Sister M Maelisa, Sister Patricia Columba and Sister Joseph Comgall, from Hammersmith, London were joined at Melbourne by Sister Francis Clare (Mother Superior) and Sister Mary Margaret from Ballarat. On first arriving in Brisbane on 3 March 1921, the sisters stayed for three weeks at All Hallows Convent with the Sisters of Mercy. On 4 March, they got their first glimpse of their new home - a dwelling of nine small rooms on rising ground overlooking the sea with Wynnum two miles distant on one side and Brisbane twelve miles on the other.',\n",
       "  'On Good Friday, 25 March 1921, the Sisters arrived to commence charity work at Wynnum. On 1 April, the first Mass was celebrated in the house by Archbishop Duhig, who also blessed the vestments and the house. The Reverend Dean Horan of Ipswich was the first resident. The sisters also took in and cared for seven elderly ladies. In 1922 the neighbouring property of Silversprings was purchased by Archbishop Duhig for £3500. The house and other buildings from Mt Margaret were moved to the new property where they were joined to the existing house to provide accommodation for the Sisters and those in need of care. By Christmas of 1922, the new home had 35 residents.',\n",
       "  'Within a couple of years more accommodation was urgently needed. With the help of their benefactors and supported by the Mother House in London, the task of building a new home was undertaken. Work began on the first section in 1924 with the firm Hennessy, Hennessy, Keesing & Co and JP Donoghue, engaged by Archbishop Duhig, designing the new building. The tender of builder George Turner was accepted for £44,200. The foundation stone of what is now the eastern wing of the present building was laid by Archbishop Duhig on 13 April 1924. The building was blessed on 15 August 1925 by Archbishop Duhig and officially opened by then Governor of Queensland, Sir Matthew Nathan, the following day. £3000 was subscribed towards the cost of the new building.',\n",
       "  \"In April 1926, the first children arrived from England to be cared for by the Sisters of Nazareth House. Local children were admitted in October the same year. The Sisters' objective was to care for orphaned or abandoned children. According to the Sisters of Nazareth House, children cared for at Wynnum during the succeeding forty years included both indigenous and non-indigenous children. Between 1889-1980 some 4000 children including orphans, wards of the State and migrant children were cared for by the Sisters of Nazareth in 8 homes across Australia and New Zealand.\",\n",
       "  'The Catholic Advocate in January 1930 described Nazareth House as a \"stately and impressive brick building...terraced lawns leading up to the front entrance and a fine group of statutory in the drive outside...Wide cloistered balconies surround the new building...The entrance hall has richly coloured leadlight windows and in the sitting-room and dining-room on either side are pictures of Nazareth House, Hammersmith, London, where the Sisters had their training...The pretty chapel in the left wing opens into the Sisters\" choir room...\"',\n",
       "  'The second stage of construction of the main building was commenced in January 1938 with the foundation stone laid by Archbishop Duhig on 20 March. The opening was held on 2 July 1939. Comprising a convent, chapel and laundry, this section was completed in April 1939 at a cost of £30,000. According to a contemporary article in the Catholic Leader, the extensions consisted of \"one million bricks.\"',\n",
       "  'In 1963, a brick building was constructed in the grounds to provide modern amenities for male residents and a new kitchen block. Two thirds of the cost of the extension was contributed by the Commonwealth Government. At the time of its official opening by Archbishop Duhig, Nazareth House was providing accommodation for 85 \"senior citizens\" and 70 children.',\n",
       "  \"In 1982 Nazareth House ceased its function as a care facility for children with the move towards placing children in need with foster families. The need for a nursing home unit had long been required and it was decided that a new Nursing Home should be built on the area at the rear of the main building where the children's playground had been, and that the original building should be renovated to make single room accommodation, with ensuite facilities, for hostel residents.\",\n",
       "  'The new Nursing Home was blessed by the Most Reverend James Cuskelly, Auxiliary Bishop of Brisbane and opened by Senator, The Hon. Don Grimes, Minister for Social Security, on Sunday 17 June 1984. The new complex connected to the old building via covered passageways and provided four 4-bed wards, seven 2-bed wards and five single rooms. Work on renovation of the old building commenced in February 1985 and comprised the construction of 42 single rooms with ensuites.',\n",
       "  'Nazareth House continues to operate as an aged care facility.',\n",
       "  \"Nazareth House is prominently located on an elevation along Wynnum North Road. The complex consists of a number of buildings including the original building (St Mary's), the Convent and Chapel and two more recent additions, St Joseph's Hostel (1960s) and the nursing home known as Larmeniere (1980s). Nazareth House has sweeping views across the north-east to Moreton Bay.\",\n",
       "  'St Mary\\'s is an H-shaped two storey brick building with basement and a terracotta tiled roof. Verandahs, some of which have been enclosed along the northern and eastern facades on both the ground and first floors, surround the building. A centrally located projecting entrance is situated along the northern facade of the building. The central section of the entrance rises above the roof line forming a parapet. A recess, in which sits a large rendered statue, is located in the upper section of the facade. The statue and recess are surrounded by rendered high relief moulding. Below the statue are the words \"NAZARETH HOUSE\". The parapet is surmounted by a rendered Celtic cross and rendered pedestal. Decorative relief work is located along the parapet and on the piers on either side of the central section. The first floor balcony, where it forms part of the projecting entrance, has a large, rounded arch with a moulded keystone. The ground floor verandah, similarly to the rest of the building, has decorative blue brickwork, including blue brick crosses centrally located along the entire facades.',\n",
       "  \"On the northern facade of St Mary's, the western and eastern end of the projecting wings have parapets surmounted by rendered Celtic crosses on a rendered pedestal. This facade as the same decorative blue brickwork and decorative relief work on the underside of the parapet. The western facade, the former rear entrance, is now the main entrance to the building. The gabled parapet has a row of dentils on the underside and a group of five windows, four are narrow and rectangular in shape and the central window is larger with a round arch opening with moulded decorative detail. The parapet is surmounted by a rendered pedestal and Celtic cross. The verandahs on the ground and first floors have been enclosed and the original doorway has been widened and replaced with automatic doors. A number of other paired timber doors are located along the verandahs on both the ground and first floors. The doors are panelled and have glass breezeway and fanlight assemblies. The foundation stone is located near the front entrance along the northern facade of the building.\",\n",
       "  \"Internally, the ground and first floors of St Mary's are similar in plan. The entrance is made up of an elaborate set of paired timber, panelled doors with large fanlight and breezeway assemblies which contain coloured glass and leadlighting. The front entrance of St Mary's, along the northern facade opens to an entry foyer with a pressed metal ceiling and timber panelled doorway and architraves. Rooms open on either side of the entrance foyer. A corridor runs east–west through the building. A turning, timber staircase with an ornate timber newel and timber rails is located off the corridor on the southern side of the central wing of the building. Rooms for the residents are located on both floors. Rooms containing offices open off the corridor. Pressed metal ceilings remain in many of the rooms including the dining room located in the south-western wing and the hall in the north-western wing. A staircase at the eastern end of the building has been isolated to form a fire stair.\",\n",
       "  'The convent is a U-shaped two-storey brick building with a basement level. The building has a broken-back roof clad with terracotta tiles. The building has a centrally located projecting entrance with a gabled roof, along the northern elevation. The central section of the entrance rises above the roof line forming a parapet. The parapet is surmounted by a rendered Celtic cross. Decorative relief work is located along the parapet and on the piers on either side of the central section. Paired round arched openings are located in the upper section of the facade. The openings are surrounded by decorative blue brickwork. The convent has verandahs to the ground and first floors and, similarly to the projecting entrance, the orange brickwork is surrounded by decorative blue brickwork. Along the ground level verandah has rounded arches and the first floor verandah squared arches. Double hung sash windows, some of which have been screened, are located along the entire facades of the convent.',\n",
       "  \"Internally, the entrance foyer has a pressed metal ceiling and parquetry floor. Large double, timber, panelled doors with coloured glass and leadlighting with breezeway assembly, open to the central corridor. The hallway is rendered with decorative moulded detail along the wall. The Sister's rooms are simple in plan and decorative detail. The rooms have timber doors and architraves with breezeway assembly, painted walls and austere ceilings. Some are slightly larger than others. Hallways leading to bathroom facilities, along with the bathrooms, have terrazzo floors. The modern kitchen is located on the ground floor.\",\n",
       "  \"The chapel is built of brick with a gabled roof at the northern end and a clad with terracotta tiles. The entrance to the chapel is located in the southern facade. Also located in this facade is the chapel's foundation stone. The gabled parapet has decorative relief work the underside and a group of three round arched windows, A smaller, narrow, rectangular window with louvres is located above these. The parapet is surmounted by a rendered Celtic cross. The chapel is fitted with copper gutters and downpipes.\",\n",
       "  'Internally, the chapel is rendered with a marble altar at the northern end of the building. A number of stained glass windows, as well as timber pews, are located in the chapel. A window opens in the upper section of the west wall of the chapel, this same window forms part of the convent wall. From this window, Sisters within the convent are able to have a view of the altar.',\n",
       "  'A rendered statue on a concrete pedestal, depicting the Holy Family, the gift of Thomas Mahon and a grotto are located in front of the northern elevation of St Mary\\'s. The entrance gates are brick with a metal archway. The archway contains the words \"NAZARETH HOUSE\" spelt out with original metal lettering.',\n",
       "  \"The later buildings, St Josephs and Nursing Hostel, are single storey brick buildings and are located to the south of St Mary's and the convent and chapel. Covered walkways join St Mary's with both buildings. Other buildings which form part of the complex include plant rooms, a laundry and a change room.\",\n",
       "  'Nazareth House was listed on the Queensland Heritage Register on 2 April 2002 having satisfied the following criteria.',\n",
       "  \"The place is important in demonstrating the evolution or pattern of Queensland's history.\",\n",
       "  \"Nazareth House is significant for its association with the establishment of the Poor Sisters of Nazareth in Queensland and the religious and social practices they implemented. The establishment of Nazareth House reflects particularly the active interest of James Duhig and his confident building program which saw the establishment of many of the Catholic Church's prominent buildings during his time as Archbishop of Brisbane.\",\n",
       "  'The place is important in demonstrating the principal characteristics of a particular class of cultural places.',\n",
       "  \"Prominently located along Wynnum North Road and visible for some distance from Wynnum, Nazareth House is significant for its considerable architectural merit, and landmark value, and is an important example of the works of one Brisbane's premier architectural practitioners and firms, Hennessy & Hennessy, Keesing and Co and JP Donoghue, who undertook many projects for the Catholic Church during Archbishop Duhig's time.\",\n",
       "  'The place is important because of its aesthetic significance.',\n",
       "  \"Prominently located along Wynnum North Road and visible for some distance from Wynnum, Nazareth House is significant for its considerable architectural merit, and landmark value, and is an important example of the works of one Brisbane's premier architectural practitioners and firms, Hennessy & Hennessy, Keesing and Co and JP Donoghue, who undertook many projects for the Catholic Church during Archbishop Duhig's time.\",\n",
       "  'The place has a strong or special association with a particular community or cultural group for social, cultural or spiritual reasons.',\n",
       "  'Nazareth House is significant for its association with those, both past and present, who have lived at the house.',\n",
       "  \"The place has a special association with the life or work of a particular person, group or organisation of importance in Queensland's history.\",\n",
       "  \"Nazareth House is significant for its association with the establishment of the Poor Sisters of Nazareth in Queensland and the religious and social practices they implemented. The establishment of Nazareth House reflects particularly the active interest of James Duhig and his confident building program which saw the establishment of many of the Catholic Church's prominent buildings during his time as Archbishop of Brisbane.\",\n",
       "  'This Wikipedia article was originally based on \"The Queensland heritage register\" published by the State of Queensland under CC-BY 3.0 AU licence (accessed on 7 July 2014, archived on 8 October 2014). The geo-coordinates were originally computed from the \"Queensland heritage register boundaries\" published by the State of Queensland under CC-BY 3.0 AU licence (accessed on 5 September 2014, archived on 15 October 2014).',\n",
       "  'Media related to Nazareth House, Wynnum at Wikimedia Commons']}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge accuracy:  0.7226458591948927\n"
     ]
    }
   ],
   "source": [
    "knowledge_correct = 0\n",
    "\n",
    "for item in valid_dataset:\n",
    "    knowledge_candidates = item[\"knowledge_candidates\"]\n",
    "    knowledge_index = item[\"knowledge_answer_index\"]\n",
    "    knowledge = item[\"knowledge\"]\n",
    "    predict_index = 0\n",
    "    stop = False\n",
    "\n",
    "    for i, candidate in enumerate(knowledge_candidates):\n",
    "        for knowledge_item in knowledge:\n",
    "            if candidate in knowledge_item:\n",
    "                predict_index = i\n",
    "                stop = True\n",
    "                break\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "    if predict_index == knowledge_index:\n",
    "        knowledge_correct += 1\n",
    "  \n",
    "    # break\n",
    "print(\"Knowledge accuracy: \", knowledge_correct / len(valid_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
