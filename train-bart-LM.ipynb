{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from transformers import AutoTokenizer, BartTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "class TF_IDF:\n",
    "\tdef __init__(self, \n",
    "\t\t\tcorpus: List[List[int]],\n",
    "\t\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tExample usage:\n",
    "\t\t\tcorpus = [\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2],\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\ttf_idf = TF_IDF(corpus)\n",
    "\t\t\t\n",
    "\t\t\tsimilar_sentences = tf_idf.get_similar([1, 2, 3], n=3)\n",
    "\t\t\t>>> similar_sentences\n",
    "\t\t\t[\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2]\n",
    "\t\t\t]\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tcorpus (List[List[int]]): токенизированный корпус\n",
    "\t\t\"\"\"\n",
    "\t\tself.vectorizer = TfidfVectorizer(\n",
    "\t\t\t# token_pattern is number\n",
    "\t\t\ttoken_pattern=r\"(?u)\\b\\d+\\b\", \n",
    "\t\t)\n",
    "\t\tnew_corpus = self.__encode_sentences(corpus)\n",
    "\n",
    "\t\tself.X = self.vectorizer.fit_transform(new_corpus)\n",
    "\t\tself.corpus = corpus\n",
    "\t\n",
    "\tdef __encode_sentence(self, sentence: List[int]) -> str:\n",
    "\t\treturn \" \".join(list(map(str, sentence)))\n",
    "\n",
    "\tdef __encode_sentences(self, sentences: List[List[int]]) -> List[str]:\n",
    "\t\treturn list(map(self.__encode_sentence, sentences))\n",
    "\t\n",
    "\tdef top_similar(self, \n",
    "\t\t\tquery: List[List[int]] = None,\n",
    "\t\t\ttop_k: int = 1,\n",
    "\t\t) -> List[List[int]]:\n",
    "\t\tquery = self.__encode_sentences(query)\n",
    "\t\tquery = self.vectorizer.transform(query)\n",
    "\t\t\n",
    "\t\tsimilarity = cosine_similarity(self.X, query)\n",
    "\t\tsimilarity = similarity.flatten()\n",
    "\t\tsimilarity = np.argsort(similarity)[::-1][:top_k]\n",
    "\t\tsimilarity = similarity.tolist()\n",
    "\n",
    "\t\tsimilar_samples = [ self.corpus[i] for i in similarity ]\n",
    "\t\treturn similar_samples\n",
    "\n",
    "\n",
    "class FoCusDatasetSampleV1:\n",
    "\t__slots__ = (\n",
    "\t\t'persona', \n",
    "\t\t'knowledge_candidates',  \n",
    "\t\t'persona_grounding', \n",
    "\t\t'dialog', \n",
    "\t\t'knowledge_answer_index',\n",
    "\t\t\"knowledge\"\n",
    "\t)\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\tpersona: List[str],\n",
    "\t\t\tknowledge_candidates: List[str],\n",
    "\t\t\tpersona_grounding: List[int],\n",
    "\t\t\tdialog: List[str],\n",
    "\t\t\tknowledge: List[str],\n",
    "\t\t\tknowledge_answer_index: int,\n",
    "\t\t) -> None:\n",
    "\t\tself.persona = persona\n",
    "\t\tself.knowledge_candidates = knowledge_candidates\n",
    "\t\tself.persona_grounding = persona_grounding\n",
    "\t\tself.knowledge_answer_index = knowledge_answer_index\n",
    "\t\tself.dialog = dialog\n",
    "\t\tself.knowledge = knowledge\n",
    "\t\n",
    "\tdef get_dict(self) -> dict:\n",
    "\t\treturn {\n",
    "\t\t\t'persona': self.persona,\n",
    "\t\t\t'knowledge_candidates': self.knowledge_candidates,\n",
    "\t\t\t'persona_grounding': self.persona_grounding,\n",
    "\t\t\t'dialog': self.dialog,\n",
    "\t\t\t'knowledge_answer_index': self.knowledge_answer_index,\n",
    "\t\t\t'knowledge': self.knowledge,\n",
    "\t\t}\n",
    "\n",
    "class FoCusDatasetV1:\n",
    "\tdef __init__(self,\n",
    "\t\tinput_dataset_path: str = None,\n",
    "\t\t) -> None:\n",
    "\t\tassert input_dataset_path is not None, 'input_dataset_path is None'\n",
    "\n",
    "\t\tself.input_dataset_path = input_dataset_path\n",
    "\t\tself.dataset: List[FoCusDatasetSampleV1] = []\n",
    "\n",
    "\t\tself.__build_dataset()\n",
    "\t\n",
    "\tdef __build_dataset(self) -> None:\n",
    "\t\tinitial_train_dataset = self.__read_dataset(self.input_dataset_path)\n",
    "\t\tself.dataset = self.__create_initial_dataset(initial_train_dataset)\n",
    "\t\n",
    "\tdef __create_initial_dataset(self, initial_dataset: Dict) -> List[FoCusDatasetSampleV1]:\n",
    "\t\tdataset = []\n",
    "\t\tinitial_dataset_data = initial_dataset['data']\n",
    "\t\t\n",
    "\t\tfor i, dialog_set in enumerate(initial_dataset_data):\n",
    "\t\t\tpersona = dialog_set['persona']\n",
    "\t\t\tutterances = dialog_set['utterance']\n",
    "\t\t\tknowledge = dialog_set['knowledge']\n",
    "\t\t\t\n",
    "\t\t\tfor j, utterance in enumerate(utterances):\n",
    "\t\t\t\tpersona_grounding = list(map(int, utterance['persona_grounding']))\n",
    "\t\t\t\tknowledge_candidates = utterance['knowledge_candidates']\n",
    "\t\t\t\tknowledge_answer_index = utterance['knowledge_answer_index']\n",
    "\t\t\t\tdialog_index_key = [item for item in utterance.keys() if 'dialog' in item][0]\n",
    "\t\t\t\tdialog = utterance[dialog_index_key]\n",
    "\t\t\t\t\n",
    "\t\t\t\tdata_sample = FoCusDatasetSampleV1(\n",
    "\t\t\t\t\tpersona=persona,\n",
    "\t\t\t\t\tknowledge_candidates=knowledge_candidates,\n",
    "\t\t\t\t\tpersona_grounding=persona_grounding,\n",
    "\t\t\t\t\tdialog=dialog,\n",
    "\t\t\t\t\tknowledge_answer_index=knowledge_answer_index,\n",
    "\t\t\t\t\tknowledge=knowledge,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tdata_sample = data_sample.get_dict()\n",
    "\t\t\t\tdataset.append(data_sample)\n",
    "\t\t\n",
    "\t\treturn dataset\n",
    "\t\n",
    "\tdef __read_dataset(self, input_path: str) -> list:\n",
    "\t\twith open(input_path, 'r') as f:\n",
    "\t\t\tdataset = json.load(f)\n",
    "\t\treturn dataset\n",
    "\n",
    "class PytorchFoCusDatasetV1(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\tdataset: FoCusDatasetV1,\n",
    "\t\t) -> None:\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\t\tself.bart_hyperparameters = BartFoCusDatasetSampleHyperparameters()\n",
    "\t\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleV1:\n",
    "\t\tdataset_sample: FoCusDatasetSampleV1 = self.dataset[index]\n",
    "\t\ttrain_sample: BartFoCusDatasetSampleV1 = BartFoCusDatasetSampleV1(\n",
    "\t\t\tfocus_dataset_sample=dataset_sample,\n",
    "\t\t\ttokenizer=self.bart_tokenizer,\n",
    "\t\t\thyperparameters=self.bart_hyperparameters,\n",
    "\t\t)\n",
    "\t\treturn train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'persona': [\n",
    "\t'I would like to visit the Nazareth House again.',\n",
    "\t'I love Benevolent institutions.',\n",
    "\t'I am interested in History.',\n",
    "\t'I have curiosity about the Description of this place.',\n",
    "\t'I would like to know when it was Built.'\n",
    "],\n",
    " 'knowledge_candidates': [\n",
    "\t'Nazareth House is a heritage-listed benevolent institution at 272 Wynnum North Road, Wynnum, City of Brisbane, Queensland, Australia.',\n",
    "  \t'However, in many cases, a hearing is not held.',\n",
    "  \t'The church and school buildings are listed together as a Cleveland Designated Landmark.',\n",
    "  \t\"Until the reorganisation of London's local government in 1965, Muswell Hill formed part of the Borough of Hornsey within the administrative county of Middlesex.\",\n",
    "  \t'This operation enabled the Canadian Sulpicians to expand their primary work, the education of priests.',\n",
    "  \t\"Bosworth's design was heavily Greek-influenced: though the facade is made of white Vermont granite, it features layers of gray granite columns in Doric and Ionic styles, as well as various Greek-inspired ornamentation.\",\n",
    "  \t'The Insurance Hall is designated as a Grade II listed building, in part due to these murals.',\n",
    "  \t'It has been pointed out that this need could have been met with the man-made Stagnum (lake) of Agrippa or, more likely, the Euripus (canal) which allowed for runoff from the Stagnum to flow into the Tiber (please see below for more information on both the Stagnum and the Euripus).',\n",
    "  \t'By 1217, documents show that the castle at Almeida is one of several strong points that guard the border between Spain and Portugal.',\n",
    "  \t'The Riverwalk runs along much of the Brisbane River foreshore throughout the inner-city area, with the longest span running between Newstead and Toowong.'],\n",
    " 'persona_grounding': [1, 0, 0, 0, 0],\n",
    " 'dialog': [\n",
    "\t\"I think I've been there before but I don't remember the name of this place.\",\n",
    "  \t'This place is the Nazareth House, which you would like to visit again.'],\n",
    " 'knowledge_answer_index': 0\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Trasimeno's castles?\n",
      "There are castles all around Trasimeno, many in the center of small towns while others are isolated and in ruins. Castiglione del Lago, Passignano, Magione, Maggiore, and Polvese islands all have castles, while Zocco castle, Montali castle, and others are on hilltops.\n"
     ]
    }
   ],
   "source": [
    "temp = FoCusDatasetV1(input_dataset_path='./datasets/FoCus/valid_focus.json')\n",
    "temp = temp.dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 3, 4]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "corpus = [\n",
    "    [1, 2, 3, 4],\n",
    "\t[1, 2, 3],\n",
    "\t[1, 2, ]\n",
    "]\n",
    "\n",
    "feature = [\n",
    "\t[1, 2, 3],\n",
    "]\n",
    "\n",
    "tf_idf = TF_IDF(corpus=corpus)\n",
    "tf_idf.top_similar(query=feature, top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartFoCusDatasetSampleHyperparameters:\n",
    "\tdef __init__(self,\n",
    "\t\t\tdialog_history_length: int = 1,\n",
    "\t\t\tcontext_length: int = 1,\n",
    "\t\t\tknowledge_length: int = 1,\n",
    "\t\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdialog_history_length (int): количество пар диалогов(назад), которые будут \n",
    "\t\t\t\tиспользоваться для генерации ответа\t\n",
    "\t\t\tcontext_length (int): количество предложений из диалога, относительно которых \n",
    "\t\t\t\tбудут выбираться похожие из поля knowledge\n",
    "\t\t\tknowledge_length (int): количество предложений из knowledge, которые будут\n",
    "\t\t\t\tподаваться на вход модели \n",
    "\t\t\"\"\"\n",
    "\t\tself.dialog_history_length = 1\n",
    "\t\tself.context_length = 1\n",
    "\t\tself.knowledge_length = 1\n",
    "\n",
    "class BartFoCusDatasetSampleV1:\n",
    "\t\"\"\"\n",
    "\tв этом датасете будет просто языковое моделирование\n",
    "\tс вставкой информацией о персоне и базы знаний + сами диалоги.\n",
    "\t- предложение из персоны будет вставляться только то что использовалось для генерации ответа\n",
    "\t- с предложением из базы знаний аналогично\n",
    "\t- knowledge это предложения из базы знаний отобранные при помощи tf-idf(похожие на вопрос пользователя) \n",
    "\tпримерно так:\n",
    "\t\t[BOS] [persona] [SEP] [knowledge] [SEP] [dialog] [SEP] \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\tfocus_dataset_sample: FoCusDatasetSampleV1,\n",
    "\t\t\ttokenizer: BartTokenizer,\n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparameters,\n",
    "\t\t) -> None:\n",
    "\t\tself.focus_dataset_sample = focus_dataset_sample\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\n",
    "\t\tself.bos_token_id = self.tokenizer.bos_token\n",
    "\t\tself.pad_token_id = self.tokenizer.pad_token\n",
    "\t\tself.unk_token_id = self.tokenizer.unk_token\n",
    "\t\tself.sep_token_id = self.tokenizer.sep_token\n",
    "\t\tself.cls_token_id = self.tokenizer.cls_token\n",
    "\t\n",
    "\tdef __flat_list(self, list_of_lists: List[List]) -> List:\n",
    "\t\treturn list(chain.from_iterable(list_of_lists))\n",
    "\n",
    "\tdef get_dict(self) -> dict:\n",
    "\t\tdialog_history_length = self.hyperparameters.dialog_history_length\n",
    "\t\tcontext_length = self.hyperparameters.context_length\n",
    "\t\tknowledge_length = self.hyperparameters.knowledge_length\n",
    "\n",
    "\t\tencoded_persona = self.tokenizer.batch_encode_plus(temp['persona'], add_special_tokens=False)\n",
    "\n",
    "\t\tdialog_history = temp['dialog'][-2*dialog_history_length:]\n",
    "\t\tdialog_history_feature = self.tokenizer.batch_encode_plus(dialog_history[:-1], add_special_tokens=False)\n",
    "\t\tdialog_history_target = self.tokenizer.batch_encode_plus(dialog_history[-1:], add_special_tokens=False)\n",
    "\n",
    "\t\ttrue_knowledge_answer = [ temp['knowledge_candidates'][temp['knowledge_answer_index']] ]\n",
    "\t\ttrue_knowledge_answer = self.tokenizer.batch_encode_plus(true_knowledge_answer, add_special_tokens=False)\n",
    "\n",
    "\t\tknowledge_candidates = temp['knowledge_candidates']\n",
    "\t\tknowledge_candidates = self.tokenizer.batch_encode_plus(knowledge_candidates, add_special_tokens=False)\n",
    "\n",
    "\t\tquery_context = dialog_history_feature['input_ids'][-context_length:]\n",
    "\t\tknowledge = self.tokenizer.batch_encode_plus(temp['knowledge'], add_special_tokens=False)\n",
    "\t\t\n",
    "\t\ttf_idf = TF_IDF(corpus=knowledge_candidates['input_ids'])\n",
    "\t\tmost_similar_knowledge = tf_idf.top_similar(\n",
    "\t\t\tquery=query_context,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# [BOS] [persona] [SEP] [knowledge] [SEP] [dialog] [SEP]\n",
    "\t\tflat_persona = self.__flat_list(encoded_persona['input_ids'])\n",
    "\t\tflat_knowledge = self.__flat_list(most_similar_knowledge)\n",
    "\t\tflat_dialog_history = self.__flat_list(dialog_history_feature['input_ids'])\n",
    "\t\tinput_sequence = [\n",
    "\t\t\tself.bos_token_id,\n",
    "\t\t\t*flat_persona,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_knowledge,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_dialog_history,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t]\n",
    "\t\t# TODO: разобраться как происходит генерация следующего токена\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain.from_iterable([ [1,2,3], [4,5,6] ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*[1, 2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
