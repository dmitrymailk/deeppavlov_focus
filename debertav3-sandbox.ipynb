{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /home/dimweb/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /home/dimweb/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/dimweb/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 5365, 447, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout, DebertaV2Model, ContextPooler\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from typing import Optional, List, Dict, Any, Union, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DebertaV3ForClassification(DebertaV2ForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        num_labels = getattr(config, \"num_labels\", 2)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.pooler = ContextPooler(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "        drop_out = getattr(config, \"cls_dropout\", None)\n",
    "        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
    "        self.dropout = StableDropout(drop_out)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = self.pooler(encoder_layer)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = loss_fn(logits, labels.view(-1))\n",
    "            \n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states, \n",
    "            attentions=outputs.attentions\n",
    "        )\n",
    "\n",
    "# model = DebertaV3ForClassification.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer.encode(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from core.hyperparameters.debertav3_hyperparameters import DebertaV3HyperparametersV1\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from core.dataloaders.focus.lighting.debertav3_lighting_dataloaders import DebertaV3FoCusLightningDataModuleV1, DebertaV3FoCusLightningDataModuleV2 \n",
    "\n",
    "hyperparameters = DebertaV3HyperparametersV1(\n",
    "        train_batch_size=16,\n",
    "        valid_batch_size=16,\n",
    "    )\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\n",
    "        hyperparameters.model_name,\n",
    "    )\n",
    "\n",
    "data_module = DebertaV3FoCusLightningDataModuleV2(\n",
    "        train_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "        valid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        tokenizer=tokenizer,  # type: ignore\n",
    "        debug_status=0,\n",
    "    )\n",
    "    \n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   279, 10367,  ...,     0,     0,     0],\n",
       "         [    1,   606,   307,  ...,     0,     0,     0],\n",
       "         [    1, 10420, 16479,  ..., 15713,   302,     2],\n",
       "         ...,\n",
       "         [    1,   279, 23409,  ...,     0,     0,     0],\n",
       "         [    1,   279, 75313,  ...,     0,     0,     0],\n",
       "         [    1,   434,   340,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'unique_ids': ['WL1G6EX8T9JY_dialogue1',\n",
       "  'NZQX869YZZK9_dialogue6',\n",
       "  'NTGAXOBEIRLQ_dialogue6',\n",
       "  'ZANR2M0FJKWF_dialogue1',\n",
       "  'EKEMEF4T3OHH_dialogue5',\n",
       "  'VTFNAHV7OIG4_dialogue5',\n",
       "  'HYOQLKN3XGCD_dialogue5',\n",
       "  'KAN28FJWSTJP_dialogue4',\n",
       "  'P878Z7YI2MMT_dialogue1',\n",
       "  'YJH7UTD0DR6A_dialogue4',\n",
       "  'K74N0MZIW09R_dialogue6',\n",
       "  'Q3EABP9MAEWV_dialogue4',\n",
       "  'DYFMDYGGVUWG_dialogue6',\n",
       "  'L4X67KG6MQ2Z_dialogue1',\n",
       "  'DKQ8U7DLSBYM_dialogue4',\n",
       "  'OEU6SDO0AGQ9_dialogue5']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_module.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from core.dataloaders.focus.models.debertav3_dataloaders import DebertaV3FoCusKnowledgeDatasetSampleV2\n",
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetKnowledgeV2\n",
    "from torch.utils.data import Dataset\n",
    "from typing import TypeVar\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from core.hyperparameters.debertav3_hyperparameters import DebertaV3HyperparametersV1\n",
    "from core.dataloaders.focus.lighting.debertav3_lighting_dataloaders import DebertaV3FoCusLightningDataModuleV4\n",
    "\n",
    "train_dataset = FoCusDatasetKnowledgeV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    "    is_train=True,\n",
    ")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "hyperparameters = DebertaV3HyperparametersV1(\n",
    "    train_batch_size=16,\n",
    "    valid_batch_size=16,\n",
    ")\n",
    "is_debug = False\n",
    "data_module = DebertaV3FoCusLightningDataModuleV4(\n",
    "    train_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "    valid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    debug_status=is_debug,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 45050,  9291,  ...,     0,     0,     0],\n",
       "         [    1,   325,   269,  ...,     0,     0,     0],\n",
       "         [    1,   279, 14825,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1,   325,   327,  ...,   470,   302,     2],\n",
       "         [    1,   279,   918,  ...,     0,     0,     0],\n",
       "         [    1,   279,  4419,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'unique_ids': ['ZY4PM4NI7BX8_dialogue6',\n",
       "  'H9VU7XIO2V8Y_dialogue6',\n",
       "  'UZTAFONAHYX9_dialogue2',\n",
       "  'N3BKTMTKN6PG_dialogue1',\n",
       "  'I0PTTIDIWCLR_dialogue5',\n",
       "  'JBWLG35559PZ_dialogue4',\n",
       "  'YPH9UCD0CZ4J_dialogue6',\n",
       "  'HXK3XQEDGK3N_dialogue2',\n",
       "  'WG8PT0YLSUFX_dialogue5',\n",
       "  'BFJXIZ89DJY9_dialogue6',\n",
       "  'NK1R15OD3U28_dialogue1',\n",
       "  'B99EPGEXYSNP_dialogue1',\n",
       "  'JIROGAE1KM0I_dialogue2',\n",
       "  'EPDJVKUVDB7T_dialogue3',\n",
       "  'FEOUHHD7BJDN_dialogue2',\n",
       "  '95FLNYPD7MKA_dialogue4']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_module.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2014)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "target = torch.ones([2], dtype=torch.float32)  # 64 classes, batch size = 10\n",
    "output = torch.full([2, 1], 1.5)  # A prediction (logit)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "print(output.shape)\n",
    "print(target.shape)\n",
    "criterion(output.view(-1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(input.shape)\n",
    "print(target.shape)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3774,  0.3921],\n",
       "         [ 1.1231,  0.4480],\n",
       "         [-0.6059, -1.7346],\n",
       "         [-0.4239, -1.7667],\n",
       "         [-1.9259, -0.9230]],\n",
       "\n",
       "        [[-0.4765, -0.1975],\n",
       "         [ 0.4054, -0.9033],\n",
       "         [-0.5289, -0.9566],\n",
       "         [-0.2321,  0.2957],\n",
       "         [-1.1701,  0.7240]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2, 5, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1],\n",
       "        [1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2, 5, 2)).argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3542\n",
      "5639\n"
     ]
    }
   ],
   "source": [
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetPersonaV2\n",
    "\n",
    "dataset = FoCusDatasetPersonaV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    "    is_train=True,\n",
    ")\n",
    "\n",
    "positive_examples = 0\n",
    "negative_examples = 0\n",
    "for sample in dataset:\n",
    "    if sample[\"persona_grounding\"] == 1:\n",
    "        positive_examples += 1\n",
    "    else:\n",
    "        negative_examples += 1\n",
    "\n",
    "print(positive_examples)\n",
    "print(negative_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this house look old to me, when it was built?\n",
      "This house is relatively old, but since you would like to know when it was built, I will explain it to you. Nazareth House was built from 1924 to 1939.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[4]['dialog'][-2])\n",
    "print(dataset[4]['dialog'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142032458337872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_examples/(positive_examples+negative_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9231\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from core.dataloaders.focus.lighting.debertav3_lighting_dataloaders import DebertaV3FoCusPersonaLightningDataModuleV2\n",
    "from core.base_models.debertav3_models import DebertaV3PersonaClassificationV2\n",
    "from transformers import DebertaV2Config, DebertaV2Tokenizer\n",
    "from core.hyperparameters.debertav3_hyperparameters import DebertaV3HyperparametersV1\n",
    "max_epochs = 1\n",
    "\n",
    "hyperparameters = DebertaV3HyperparametersV1(\n",
    "    train_batch_size=16,\n",
    "    valid_batch_size=16,\n",
    "    model_name=\"microsoft/deberta-v3-base\",\n",
    "    project_name=\"focus_persona_classification\",\n",
    ")\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\n",
    "    hyperparameters.model_name,\n",
    ")\n",
    "\n",
    "data_module = DebertaV3FoCusPersonaLightningDataModuleV2(\n",
    "    train_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "    valid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    "    debug_status=0,\n",
    ")\n",
    "base_model = DebertaV3PersonaClassificationV2(\n",
    "    config=DebertaV2Config.from_pretrained(\n",
    "        hyperparameters.model_name,\n",
    "    ),  # type: ignore\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    base_model.parameters(),\n",
    "    lr=hyperparameters.learning_rate,\n",
    "    weight_decay=hyperparameters.weight_decay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    num_labels=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n",
      "Loss: 0.005705759860575199\n",
      "Accuracy: 0.0\n",
      "Average loss: 0.10791930346749723\n",
      "Average accuracy: 0.11979166666666667\n",
      "----------\n",
      "Loss: 0.16637402772903442\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.16637402772903442\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.1664477288722992\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.1664108783006668\n",
      "Average accuracy: 0.1875\n",
      "----------\n",
      "Loss: 0.05837586522102356\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.13039920727411905\n",
      "Average accuracy: 0.14583333333333334\n",
      "----------\n",
      "Loss: 0.1655023843050003\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.13917500153183937\n",
      "Average accuracy: 0.15625\n",
      "----------\n",
      "Loss: 0.05896855145692825\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.12313371151685715\n",
      "Average accuracy: 0.1375\n",
      "----------\n",
      "Loss: 0.11309413611888885\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.1214604489505291\n",
      "Average accuracy: 0.13541666666666666\n",
      "----------\n",
      "Loss: 0.05686089023947716\n",
      "Accuracy: 0.0625\n",
      "Average loss: 0.11223194056323596\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11292505264282227\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11231857957318425\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1121257022023201\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11229714875419934\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.11290767043828964\n",
      "Accuracy: 0.125\n",
      "Average loss: 0.11235820092260837\n",
      "Average accuracy: 0.125\n",
      "----------\n",
      "Loss: 0.1657438725233078\n",
      "Accuracy: 0.1875\n",
      "Average loss: 0.11721144379539923\n",
      "Average accuracy: 0.13068181818181818\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     11\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     13\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m preds \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msigmoid(logits) \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mint()\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    162\u001b[0m           grads,\n\u001b[1;32m    163\u001b[0m           exp_avgs,\n\u001b[1;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           state_steps,\n\u001b[1;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m func(params,\n\u001b[1;32m    219\u001b[0m      grads,\n\u001b[1;32m    220\u001b[0m      exp_avgs,\n\u001b[1;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      state_steps,\n\u001b[1;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/optim/adamw.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    260\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m param\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m lr \u001b[39m*\u001b[39;49m weight_decay)\n\u001b[1;32m    265\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    266\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for step, batch in enumerate(data_module.val_dataloader()):\n",
    "        batch['labels'] = batch['labels'].float()\n",
    "        batch.pop(\"unique_ids\", None)\n",
    "        optimizer.zero_grad()\n",
    "        output = base_model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(logits) > 0.5).int().flatten()\n",
    "        labels = batch[\"labels\"].int().flatten()\n",
    "        acc = (preds == labels).float().mean().item()\n",
    "        total_accuracy += acc\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        print(f\"Accuracy: {acc}\")\n",
    "        print(f\"Average loss: {total_loss / (step + 1)}\")\n",
    "        print(f\"Average accuracy: {total_accuracy / (step + 1)}\")\n",
    "        print(\"-\"*10)\n",
    "        if step > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetPersonaV2\n",
    "train_data = []\n",
    "eval_data = []\n",
    "\n",
    "train_dataset = FoCusDatasetPersonaV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/train_focus.json\",\n",
    "    is_train=True,\n",
    ")\n",
    "\n",
    "for sample in train_dataset:\n",
    "    label = sample[\"persona_grounding\"]\n",
    "    query = sample[\"dialog\"][-2]\n",
    "    persona = sample[\"persona\"]\n",
    "    knowledge = sample[\"used_knowledge\"]\n",
    "    input_sent = f\"{persona} {knowledge} {query}\"\n",
    "    train_data.append([input_sent, label])\n",
    "\n",
    "valid_dataset = FoCusDatasetPersonaV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    "    is_train=False,\n",
    ")\n",
    "\n",
    "for sample in valid_dataset:\n",
    "    label = sample[\"persona_grounding\"]\n",
    "    query = sample[\"dialog\"][-2]\n",
    "    persona = sample[\"persona\"]\n",
    "    knowledge = sample[\"used_knowledge\"]\n",
    "    input_sent = f\"{persona} {knowledge} {query}\"\n",
    "    eval_data.append([input_sent, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Preparing train data\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.columns = [\"text\", \"labels\"]\n",
    "\n",
    "# Preparing eval data\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "eval_df.columns = [\"text\", \"labels\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n",
      "  0%|          | 0/114630 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:853\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    854\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m ClassificationModel(\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39mmodel_args\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39mtrain_model(\n\u001b[1;32m     14\u001b[0m     train_df\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m result, model_outputs, wrong_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval_model(eval_df)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:619\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         train_examples \u001b[39m=\u001b[39m (\n\u001b[1;32m    616\u001b[0m             train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    617\u001b[0m             train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    618\u001b[0m         )\n\u001b[0;32m--> 619\u001b[0m     train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    620\u001b[0m         train_examples, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    622\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    623\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    624\u001b[0m     train_dataset,\n\u001b[1;32m    625\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    626\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    627\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1827\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[39mreturn\u001b[39;00m dataset\n\u001b[1;32m   1826\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1827\u001b[0m     dataset \u001b[39m=\u001b[39m ClassificationDataset(\n\u001b[1;32m   1828\u001b[0m         examples,\n\u001b[1;32m   1829\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m   1830\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m   1831\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1832\u001b[0m         multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m   1833\u001b[0m         output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m   1834\u001b[0m         no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m   1835\u001b[0m     )\n\u001b[1;32m   1836\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m build_classification_dataset(\n\u001b[1;32m    283\u001b[0m         data, tokenizer, args, mode, multi_label, output_mode, no_cache\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[39m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[39m+\u001b[39m chunksize], \u001b[39mNone\u001b[39;00m, tokenizer, args\u001b[39m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[39mwith\u001b[39;00m Pool(args\u001b[39m.\u001b[39mprocess_count) \u001b[39mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m    250\u001b[0m             tqdm(\n\u001b[1;32m    251\u001b[0m                 p\u001b[39m.\u001b[39;49mimap(preprocess_data_multiprocessing, data),\n\u001b[1;32m    252\u001b[0m                 total\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(text_a),\n\u001b[1;32m    253\u001b[0m                 disable\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49msilent,\n\u001b[1;32m    254\u001b[0m             )\n\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    257\u001b[0m     examples \u001b[39m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[39m.\u001b[39mcat([example[key] \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m examples[\u001b[39m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:858\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 858\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    859\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(\n",
    "    num_train_epochs=2,\n",
    "    no_save=True\n",
    ")\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(\n",
    "    train_df\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
