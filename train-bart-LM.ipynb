{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type     | Params\n",
      "-----------------------------------\n",
      "0 | model | BartLMV1 | 178 M \n",
      "-----------------------------------\n",
      "178 M     Trainable params\n",
      "0         Non-trainable params\n",
      "178 M     Total params\n",
      "712.108   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/18288 [00:00<?, ?it/s]                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 38/18288 [00:09<1:18:30,  3.87it/s, v_num=hrow, train_loss_step=8.300]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 39/18288 [00:09<1:17:52,  3.91it/s, v_num=hrow, train_loss_step=8.030]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3775 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 79/18288 [00:19<1:13:37,  4.12it/s, v_num=hrow, train_loss_step=7.260]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 115/18288 [00:27<1:12:28,  4.18it/s, v_num=hrow, train_loss_step=7.290]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Dict, TypedDict, Optional\n",
    "import json\n",
    "from transformers import AutoTokenizer, BartTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# import datasets\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "\tAutoConfig,\n",
    "\tAutoModelForSequenceClassification,\n",
    "\tAutoTokenizer,\n",
    "\tget_linear_schedule_with_warmup,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import os\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BartModel, BartConfig, BartPretrainedModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "class TF_IDF:\n",
    "\tdef __init__(self, \n",
    "\t\t\tcorpus: List[List[int]],\n",
    "\t\t) -> None:\n",
    "\t\tr\"\"\"\n",
    "\t\tExample usage:\n",
    "\t\t\tcorpus = [\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2],\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\ttf_idf = TF_IDF(corpus)\n",
    "\t\t\t\n",
    "\t\t\tsimilar_sentences = tf_idf.get_similar([1, 2, 3], n=3)\n",
    "\t\t\t>>> similar_sentences\n",
    "\t\t\t[\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2]\n",
    "\t\t\t]\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tcorpus (List[List[int]]): токенизированный корпус\n",
    "\t\t\"\"\"\n",
    "\t\tself.vectorizer = TfidfVectorizer(\n",
    "\t\t\t# token_pattern is number\n",
    "\t\t\ttoken_pattern=r\"(?u)\\b\\d+\\b\", \n",
    "\t\t)\n",
    "\t\tnew_corpus = self.__encode_sentences(corpus)\n",
    "\n",
    "\t\tself.X = self.vectorizer.fit_transform(new_corpus)\n",
    "\t\tself.corpus = corpus\n",
    "\t\n",
    "\tdef __encode_sentence(self, sentence: List[int]) -> str:\n",
    "\t\treturn \" \".join(list(map(str, sentence)))\n",
    "\n",
    "\tdef __encode_sentences(self, sentences: List[List[int]]) -> List[str]:\n",
    "\t\treturn list(map(self.__encode_sentence, sentences))\n",
    "\t\n",
    "\tdef top_similar(self, \n",
    "\t\t\tquery: List[List[int]] = None,\n",
    "\t\t\ttop_k: int = 1,\n",
    "\t\t) -> List[List[int]]:\n",
    "\t\tquery = self.__encode_sentences(query)\n",
    "\t\tquery = self.vectorizer.transform(query)\n",
    "\t\t\n",
    "\t\tsimilarity = cosine_similarity(self.X, query)\n",
    "\t\tsimilarity = similarity.flatten()\n",
    "\t\tsimilarity = np.argsort(similarity)[::-1][:top_k]\n",
    "\t\tsimilarity = similarity.tolist()\n",
    "\n",
    "\t\tsimilar_samples = [self.corpus[i] for i in similarity]\n",
    "\t\treturn similar_samples\n",
    "\n",
    "class FoCusTF_IDF(TF_IDF):\n",
    "\tdef __init__(self,\n",
    "\t\t**kwargs,\n",
    "\t) -> None:\n",
    "\t\tsuper().__init__(**kwargs)\n",
    "\n",
    "\t\tself.cached_similar = {}\n",
    "\t\n",
    "\tdef top_similar(self, \n",
    "\t\t\tquery: List[List[int]] = None, \n",
    "\t\t\ttop_k: int = 1\n",
    "\t\t) -> List[List[int]]:\n",
    "\t\tquery_str = str(query)\n",
    "\n",
    "\t\tif query_str in self.cached_similar:\n",
    "\t\t\treturn self.cached_similar[query_str]\n",
    "\t\t\n",
    "\t\tsimilar_samples = super().top_similar(\n",
    "\t\t\tquery=query,\n",
    "\t\t\ttop_k=top_k,\n",
    "\t\t)\n",
    "\t\tself.cached_similar[query_str] = similar_samples\n",
    "\n",
    "\t\treturn similar_samples\n",
    "class FoCusDatasetSampleDictV1(TypedDict):\n",
    "\tpersona: List[str]\n",
    "\tknowledge_candidates: List[str]\n",
    "\tpersona_grounding: List[int]\n",
    "\tdialog: List[int]\n",
    "\tknowledge_answer_index: int\n",
    "\tknowledge: List[str]\n",
    "\n",
    "class FoCusDatasetSampleV1:\n",
    "\t__slots__ = (\n",
    "\t\t'persona', \n",
    "\t\t'knowledge_candidates',  \n",
    "\t\t'persona_grounding', \n",
    "\t\t'dialog', \n",
    "\t\t'knowledge_answer_index',\n",
    "\t\t\"knowledge\"\n",
    "\t)\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\tpersona: List[str],\n",
    "\t\t\tknowledge_candidates: List[str],\n",
    "\t\t\tpersona_grounding: List[int],\n",
    "\t\t\tdialog: List[str],\n",
    "\t\t\tknowledge: List[str],\n",
    "\t\t\tknowledge_answer_index: int,\n",
    "\t\t) -> None:\n",
    "\t\tself.persona = persona\n",
    "\t\tself.knowledge_candidates = knowledge_candidates\n",
    "\t\tself.persona_grounding = persona_grounding\n",
    "\t\tself.knowledge_answer_index = knowledge_answer_index\n",
    "\t\tself.dialog = dialog\n",
    "\t\tself.knowledge = knowledge\n",
    "\t\n",
    "\tdef get_dict(self) -> FoCusDatasetSampleDictV1:\n",
    "\t\treturn {\n",
    "\t\t\t'persona': self.persona,\n",
    "\t\t\t'knowledge_candidates': self.knowledge_candidates,\n",
    "\t\t\t'persona_grounding': self.persona_grounding,\n",
    "\t\t\t'dialog': self.dialog,\n",
    "\t\t\t'knowledge_answer_index': self.knowledge_answer_index,\n",
    "\t\t\t'knowledge': self.knowledge,\n",
    "\t\t}\n",
    "\n",
    "class BartFoCusDatasetSampleHyperparametersV1:\n",
    "\tdef __init__(self,\n",
    "\t\t\tdialog_history_length: int = 1,\n",
    "\t\t\tcontext_length: int = 1,\n",
    "\t\t\tknowledge_length: int = 1,\n",
    "\t\t) -> None:\n",
    "\t\tr\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdialog_history_length (int): количество пар диалогов(назад), которые будут \n",
    "\t\t\t\tиспользоваться для генерации ответа\t\n",
    "\t\t\tcontext_length (int): количество предложений из диалога, относительно которых \n",
    "\t\t\t\tбудут выбираться похожие из поля knowledge\n",
    "\t\t\tknowledge_length (int): количество предложений из knowledge, которые будут\n",
    "\t\t\t\tподаваться на вход модели \n",
    "\t\t\"\"\"\n",
    "\t\tself.dialog_history_length = 1\n",
    "\t\tself.context_length = 1\n",
    "\t\tself.knowledge_length = 1\n",
    "\t\t\n",
    "\t\tself.max_persona_tokens = 200\n",
    "\t\tself.max_dialog_history_tokens = 200\n",
    "\t\tself.max_knowledge_tokens = 200\n",
    "\t\tself.max_bot_response_tokens = 150\n",
    "\n",
    "\t\tself.dialog_bos_token = '<dialog>'\n",
    "\t\tself.dialog_eos_token = '</dialog>'\n",
    "\n",
    "\t\tself.seed = 2022\n",
    "\t\tself.train_batch_size = 4\n",
    "\t\tself.valid_batch_size = 8\n",
    "\n",
    "\t\tself.warmup_steps = 10\n",
    "\t\tself.learning_rate = 6.25e-5\n",
    "\t\tself.adam_epsilon = 1e-8\n",
    "\n",
    "\t\tself.gradient_accumulation_steps = 1\n",
    "\t\tself.train_epochs = 1\n",
    "\n",
    "\t\tself.bart_model_name = 'facebook/bart-base'\n",
    "\n",
    "class BartFoCusTokenizerV1(BartTokenizer):\n",
    "\tdef __init__(self,\n",
    "\t\t\t*args,\n",
    "\t\t\t**kwargs \n",
    "\t\t) -> None:\n",
    "\t\tsuper().__init__(**kwargs)\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_pretrained(cls, \n",
    "\t\t\t*args, \n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None, \n",
    "\t\t\t**kwargs\n",
    "\t\t):\n",
    "\t\t\n",
    "\t\ttokenizer: BartTokenizer = BartTokenizer.from_pretrained(*args, **kwargs)\n",
    "\t\t\n",
    "\t\tif hyperparameters is not None:\n",
    "\t\t\ttokens = [\n",
    "\t\t\t\thyperparameters.dialog_bos_token,\n",
    "\t\t\t\thyperparameters.dialog_eos_token,\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\ttokenizer.add_special_tokens({'additional_special_tokens': tokens})\n",
    "\n",
    "\t\treturn tokenizer\n",
    "\n",
    "class BartFoCusDatasetSampleDictV1(TypedDict):\n",
    "\tinput_ids: List[int]\n",
    "\tattention_mask: List[int]\n",
    "\n",
    "class BartFoCusDatasetSampleV1:\n",
    "\t\"\"\"\n",
    "\t[BOS][persona][SEP][knowledge][SEP][dialog][:-1][SEP]<dialog>[dialog][-1]</dialog> \n",
    "\t- [dialog] - набор диалоговых пар\n",
    "\t- persona - все предложения персоны\n",
    "\t- knowledge - топ наиболее похожих предложений из knowledge к контексту диалога\n",
    "\t- [dialog][:-1] - все диалоговые пары, кроме ответа бота\n",
    "\t- <dialog>[dialog][-1]</dialog> - ответ бота \n",
    "\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\tfocus_dataset_sample: FoCusDatasetSampleDictV1 = None,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1 = None,\n",
    "\t\t\th_params: BartFoCusDatasetSampleHyperparametersV1 = None,\n",
    "\t\t) -> None:\n",
    "\t\tself.focus_dataset_sample = focus_dataset_sample\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.h_params = h_params\n",
    "\n",
    "\t\tself.bos_token_id = self.tokenizer.bos_token_id\n",
    "\t\tself.pad_token_id = self.tokenizer.pad_token_id\n",
    "\t\tself.unk_token_id = self.tokenizer.unk_token_id\n",
    "\t\tself.sep_token_id = self.tokenizer.sep_token_id\n",
    "\t\tself.cls_token_id = self.tokenizer.cls_token_id\n",
    "\n",
    "\t\tself.dialog_bos = self.__get_token_id(h_params.dialog_bos_token)\n",
    "\t\tself.dialog_eos = self.__get_token_id(h_params.dialog_eos_token)\n",
    "\t\n",
    "\tdef __get_token_id(self, token: str) -> int:\n",
    "\t\treturn self.tokenizer.convert_tokens_to_ids(token)\n",
    "\t\n",
    "\tdef __flat_list(self, list_of_lists: List[List]) -> List:\n",
    "\t\treturn list(chain.from_iterable(list_of_lists))\n",
    "\n",
    "\tdef get_dict(self) -> BartFoCusDatasetSampleDictV1:\n",
    "\t\tdialog_history_length = self.h_params.dialog_history_length\n",
    "\t\tcontext_length = self.h_params.context_length\n",
    "\t\tknowledge_length = self.h_params.knowledge_length\n",
    "\n",
    "\t\tpersona = self.focus_dataset_sample['persona']\n",
    "\t\tdialog = self.focus_dataset_sample['dialog']\n",
    "\t\tknowledge = self.focus_dataset_sample['knowledge']\n",
    "\n",
    "\t\tencoded_persona = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tpersona, \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\n",
    "\t\tdialog_history = dialog[-2*dialog_history_length:]\n",
    "\t\tdialog_history_feature = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tdialog_history[:-1], \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\t\tdialog_history_target = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tdialog_history[-1:], \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\n",
    "\t\t# контекст на основе которого подбирается knowledge\n",
    "\t\tquery_context = dialog_history_feature['input_ids'][-context_length:]\n",
    "\t\tencoded_knowledge = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tknowledge, \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\ttf_idf = FoCusTF_IDF(corpus=encoded_knowledge['input_ids'])\n",
    "\t\tmost_similar_knowledge = tf_idf.top_similar(\n",
    "\t\t\tquery=query_context,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# [BOS][persona][SEP][knowledge][SEP][dialog][:-1][SEP]<dialog>[dialog][-1]</dialog>\n",
    "\t\tflat_persona = self.__flat_list(encoded_persona['input_ids'])\n",
    "\t\tflat_knowledge = self.__flat_list(most_similar_knowledge)\n",
    "\t\tflat_dialog_history = self.__flat_list(dialog_history_feature['input_ids'])\n",
    "\t\tflat_bot_response = self.__flat_list(dialog_history_target['input_ids'])\n",
    "\n",
    "\t\tflat_persona = flat_persona[:self.h_params.max_persona_tokens]\n",
    "\t\tflat_knowledge = flat_knowledge[:self.h_params.max_knowledge_tokens]\n",
    "\t\tflat_dialog_history = flat_dialog_history[:self.h_params.max_dialog_history_tokens]\n",
    "\t\tflat_bot_response = flat_bot_response[:self.h_params.max_bot_response_tokens]\n",
    "\n",
    "\t\tinput_sequence = [\n",
    "\t\t\tself.bos_token_id,\n",
    "\t\t\t*flat_persona,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_knowledge,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_dialog_history,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\tself.dialog_bos,\n",
    "\t\t\t*flat_bot_response,\n",
    "\t\t\tself.dialog_eos\n",
    "\t\t]\n",
    "\n",
    "\t\tattention_mask = [1] * len(input_sequence)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids': input_sequence,\n",
    "\t\t\t'attention_mask': attention_mask,\n",
    "\t\t}\n",
    "\t\t\n",
    "\n",
    "class FoCusDatasetV1:\n",
    "\tdef __init__(self,\n",
    "\t\tinput_dataset_path: str = None,\n",
    "\t\t) -> None:\n",
    "\t\tassert input_dataset_path is not None, 'input_dataset_path is None'\n",
    "\n",
    "\t\tself.input_dataset_path: str = input_dataset_path\n",
    "\t\tself.dataset: List[FoCusDatasetSampleDictV1] = []\n",
    "\n",
    "\t\tself.__build_dataset()\n",
    "\t\n",
    "\tdef __build_dataset(self) -> None:\n",
    "\t\tinitial_dataset = self.__read_dataset(self.input_dataset_path)\n",
    "\t\tself.dataset = self.__create_initial_dataset(initial_dataset=initial_dataset)\n",
    "\t\n",
    "\tdef __create_initial_dataset(self, initial_dataset: Dict = None) -> List[FoCusDatasetSampleDictV1]:\n",
    "\t\tdataset = []\n",
    "\t\tinitial_dataset_data = initial_dataset['data']\n",
    "\t\t\n",
    "\t\tfor i, dialog_set in enumerate(initial_dataset_data):\n",
    "\t\t\tpersona = dialog_set['persona']\n",
    "\t\t\tutterances = dialog_set['utterance']\n",
    "\t\t\tknowledge = dialog_set['knowledge']\n",
    "\t\t\t\n",
    "\t\t\tfor j, utterance in enumerate(utterances):\n",
    "\t\t\t\tpersona_grounding = list(map(int, utterance['persona_grounding']))\n",
    "\t\t\t\tknowledge_candidates = utterance['knowledge_candidates']\n",
    "\t\t\t\tknowledge_answer_index = utterance['knowledge_answer_index']\n",
    "\t\t\t\tdialog_index_key = [item for item in utterance.keys() if 'dialog' in item][0]\n",
    "\t\t\t\tdialog = utterance[dialog_index_key]\n",
    "\t\t\t\t\n",
    "\t\t\t\tdata_sample = FoCusDatasetSampleV1(\n",
    "\t\t\t\t\tpersona=persona,\n",
    "\t\t\t\t\tknowledge_candidates=knowledge_candidates,\n",
    "\t\t\t\t\tpersona_grounding=persona_grounding,\n",
    "\t\t\t\t\tdialog=dialog,\n",
    "\t\t\t\t\tknowledge_answer_index=knowledge_answer_index,\n",
    "\t\t\t\t\tknowledge=knowledge,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tdata_sample = data_sample.get_dict()\n",
    "\t\t\t\tdataset.append(data_sample)\n",
    "\t\t\n",
    "\t\treturn dataset\n",
    "\t\n",
    "\tdef __read_dataset(self, input_path: str) -> list:\n",
    "\t\twith open(input_path, 'r') as f:\n",
    "\t\t\tdataset = json.load(f)\n",
    "\t\treturn dataset\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleDictV1:\n",
    "\t\treturn self.dataset[index]\n",
    "\n",
    "class PytorchFoCusDatasetV1(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\t\tdataset: FoCusDatasetV1,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1 = None,\n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None,\n",
    "\t\t) -> None:\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleDictV1:\n",
    "\t\tdataset_sample = self.dataset[index]\n",
    "\t\ttrain_sample = BartFoCusDatasetSampleV1(\n",
    "\t\t\tfocus_dataset_sample=dataset_sample,\n",
    "\t\t\ttokenizer=self.tokenizer,\n",
    "\t\t\th_params=self.hyperparameters,\n",
    "\t\t)\n",
    "\t\ttrain_sample = train_sample.get_dict()\n",
    "\t\treturn train_sample\n",
    "\n",
    "\n",
    "\n",
    "class FoCusDataModuleV1(LightningDataModule):\n",
    "\tdef __init__(self, \n",
    "\t\t\ttrain_path_dataset: str = None,\n",
    "\t\t\tvalid_path_dataset: str = None,\n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1 = None,\n",
    "\t\t) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.train_path_dataset = train_path_dataset\n",
    "\t\tself.valid_path_dataset = valid_path_dataset\n",
    "\n",
    "\t\tself.train_dataset = None\n",
    "\t\tself.valid_dataset = None\n",
    "\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\n",
    "\tdef setup(self, stage: str = None) -> None:\n",
    "\t\ttrain_dataset = FoCusDatasetV1(input_dataset_path=self.train_path_dataset)\n",
    "\t\tvalid_dataset = FoCusDatasetV1(input_dataset_path=self.valid_path_dataset)\n",
    "\n",
    "\t\tself.train_dataset = PytorchFoCusDatasetV1(\n",
    "\t\t\tdataset=train_dataset,\n",
    "\t\t\ttokenizer=self.tokenizer,\n",
    "\t\t\thyperparameters=self.hyperparameters,\n",
    "\t\t)\n",
    "\t\tself.valid_dataset = PytorchFoCusDatasetV1(\n",
    "\t\t\tdataset=valid_dataset,\n",
    "\t\t\ttokenizer=self.tokenizer,\n",
    "\t\t\thyperparameters=self.hyperparameters,\n",
    "\t\t)\n",
    "\n",
    "\tdef train_dataloader(self) -> DataLoader:\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.train_dataset,\n",
    "\t\t\tbatch_size=self.hyperparameters.train_batch_size,\n",
    "\t\t\tshuffle=True,\n",
    "\t\t\tnum_workers=os.cpu_count(),\n",
    "\t\t\tcollate_fn=self.collate_fn\n",
    "\t\t)\n",
    "\t\n",
    "\tdef val_dataloader(self) -> DataLoader:\n",
    "\t\treturn DataLoader(\n",
    "\t\t\tself.valid_dataset,\n",
    "\t\t\tbatch_size=self.hyperparameters.valid_batch_size,\n",
    "\t\t\tshuffle=False,\n",
    "\t\t\tnum_workers=os.cpu_count(),\n",
    "\t\t\tcollate_fn=self.collate_fn\n",
    "\t\t)\n",
    "\t\n",
    "\tdef collate_fn(self, batch: List[FoCusDatasetSampleDictV1]) -> Dict:\n",
    "\t\t# print(\"collate_fn\", batch)\n",
    "\t\tmax_len = 0\n",
    "\t\tfor item in batch:\n",
    "\t\t\tmax_len = max(max_len, len(item['input_ids']))\n",
    "\t\t\n",
    "\t\tpad_input_ids = []\n",
    "\t\tpad_attention_mask = []\n",
    "\t\t\n",
    "\t\tfor item in batch:\n",
    "\t\t\tinput_ids: List = item['input_ids']\n",
    "\t\t\tattention_mask: List = item['attention_mask']\n",
    "\t\t\t\n",
    "\t\t\tpad_tokens = [self.tokenizer.pad_token_id] * (max_len - len(input_ids))\n",
    "\t\t\tpad_attention = [0] * (max_len - len(attention_mask))\n",
    "\t\t\t\n",
    "\t\t\tinput_ids.extend(pad_tokens)\n",
    "\t\t\tattention_mask.extend(pad_attention)\n",
    "\t\t\t\n",
    "\t\t\tpad_input_ids.append(input_ids)\n",
    "\t\t\tpad_attention_mask.append(attention_mask)\n",
    "\t\t\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids': torch.tensor(pad_input_ids),\n",
    "\t\t\t'attention_mask': torch.tensor(pad_attention_mask),\n",
    "\t\t}\n",
    "\n",
    "class BartLMV1(BartPretrainedModel):\n",
    "\tr\"\"\"\n",
    "\tSimple usage:\n",
    "\tmodel = BartLMV1(\n",
    "\t\tconfig=BartConfig.from_pretrained('facebook/bart-large'),\n",
    "\t\thyperparameters=BartFoCusDatasetSampleHyperparametersV1(),\n",
    "\t\ttokenizer=BartFoCusTokenizerV1.from_pretrained(\n",
    "\t\t\t'facebook/bart-base',\n",
    "\t\t\thyperparameters=BartFoCusDatasetSampleHyperparametersV1()),\n",
    "\t)\n",
    "\n",
    "\tinput_ids = torch.tensor([[1, 2, ]])\n",
    "\tattention_mask = torch.tensor([[1, 1,],])\n",
    "\tmodel(\n",
    "\t\tinput_ids=input_ids,\n",
    "\t\tattention_mask=attention_mask,\n",
    "\t\tlabels=input_ids,\n",
    "\t)\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\tconfig: BartConfig = None,\n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1 = None,\n",
    "\t\t) -> None:\n",
    "\t\tsuper().__init__(config=config)\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\n",
    "\t\tself.model = BartModel(config=config)\n",
    "\t\tself.lm_head = nn.Linear(config.d_model, len(tokenizer), bias=False)\n",
    "\t\n",
    "\tdef forward(self,\n",
    "\t\t\tinput_ids: torch.Tensor = None,\n",
    "\t\t\tattention_mask: torch.Tensor = None,\n",
    "\t\t\tlabels: torch.Tensor = None,\n",
    "\t\t) -> Seq2SeqLMOutput:\n",
    "\t\toutputs = self.model(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t)\n",
    "\n",
    "\t\tlogits: torch.Tensor = self.lm_head(outputs[0])\n",
    "\n",
    "\t\tloss = None\n",
    "\t\tif labels is not None:\n",
    "\t\t\t# copy from https://github.com/pkchat-focus/FoCus/blob/main/classification_modules.py#L462\n",
    "\t\t\tloss_fct = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "\t\t\tshift_logits = logits[..., :-1, :].contiguous()\n",
    "\t\t\tshift_labels = labels[..., 1:].contiguous()\n",
    "\t\t\tloss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "\t\treturn Seq2SeqLMOutput(\n",
    "\t\t\tloss=loss,\n",
    "\t\t\tlogits=logits,\n",
    "\t\t\t# не очень понимаю что это за ключ в контексте модели BART\n",
    "\t\t\tencoder_last_hidden_state=outputs[0],\n",
    "\t\t)\n",
    "\n",
    "class BARTModelV1(LightningModule):\n",
    "\tdef __init__(self, \n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1 = None,\n",
    "\t\t\tis_training: bool = False\n",
    "\t\t) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.hyperparameters = hyperparameters\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\n",
    "\t\tself.hparams.update(self.hyperparameters.__dict__)\n",
    "\n",
    "\t\tself.model = BartLMV1(\n",
    "\t\t\tconfig=BartConfig.from_pretrained(hyperparameters.bart_model_name),\n",
    "\t\t\thyperparameters=hyperparameters,\n",
    "\t\t\ttokenizer=tokenizer,\n",
    "\t\t)\n",
    "\t\tif is_training:\n",
    "\t\t\tself.model.resize_token_embeddings(len(tokenizer))\n",
    "\t\t\n",
    "\t\tself.automatic_optimization = False \n",
    "\n",
    "\t\n",
    "\tdef forward(self, \n",
    "\t\t\tinput_ids: torch.Tensor = None,\n",
    "\t\t\tattention_mask: torch.Tensor = None,\n",
    "\t\t\tlabels: torch.Tensor = None,\n",
    "\t\t) -> Seq2SeqLMOutput:\n",
    "\t\treturn self.model(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\tlabels=labels,\n",
    "\t\t)\n",
    "\t\n",
    "\tdef configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "\t\toptimizer = torch.optim.AdamW(\n",
    "\t\t\tparams=self.model.parameters(),\n",
    "\t\t\tlr=self.hyperparameters.learning_rate,\n",
    "\t\t\teps=self.hyperparameters.adam_epsilon,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tscheduler = get_linear_schedule_with_warmup(\n",
    "\t\t\toptimizer,\n",
    "\t\t\tnum_warmup_steps=self.hyperparameters.warmup_steps,\n",
    "\t\t\tnum_training_steps=self.trainer.estimated_stepping_batches,\n",
    "\t\t)\n",
    "\t\treturn [optimizer], [scheduler]\n",
    "\t\n",
    "\tdef training_step(self, batch: List, batch_idx: int) -> Dict:\n",
    "\t\tinput_ids = batch['input_ids']\n",
    "\t\tattention_mask = batch['attention_mask']\n",
    "\t\tlabels = input_ids.clone()\n",
    "\n",
    "\t\toutputs = self.model.forward(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\tlabels=labels,\n",
    "\t\t)\n",
    "\n",
    "\t\tloss = outputs.loss\n",
    "\n",
    "\t\t# https://pytorch-lightning.readthedocs.io/en/stable/common/optimization.html#id2\n",
    "\t\topt = self.optimizers()\n",
    "\t\topt.zero_grad()\n",
    "\t\tself.manual_backward(loss)\n",
    "\t\topt.step()\n",
    "\t\tsch = self.lr_schedulers()\n",
    "\t\tsch.step()\n",
    "\n",
    "\n",
    "\n",
    "\t\tself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\t\treturn {'loss': loss }\n",
    "\t\n",
    "\tdef validation_step(self, batch: List, batch_idx: int) -> Dict:\n",
    "\t\tinput_ids = batch['input_ids']\n",
    "\t\tattention_mask = batch['attention_mask']\n",
    "\t\tlabels = input_ids.clone()\n",
    "\n",
    "\t\toutputs = self.model.forward(\n",
    "\t\t\tinput_ids=input_ids,\n",
    "\t\t\tattention_mask=attention_mask,\n",
    "\t\t\tlabels=labels,\n",
    "\t\t)\n",
    "\t\tloss = outputs.loss\n",
    "\t\tself.log('valid_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "hyperparameters = BartFoCusDatasetSampleHyperparametersV1()\n",
    "tokenizer = BartFoCusTokenizerV1.from_pretrained(\n",
    "\thyperparameters.bart_model_name, \n",
    "\thyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "data_module = FoCusDataModuleV1(\n",
    "\ttrain_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "\tvalid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "\thyperparameters=hyperparameters,\n",
    "\ttokenizer=tokenizer,\n",
    ")\n",
    "model = BARTModelV1(\n",
    "\thyperparameters=hyperparameters,\n",
    "\ttokenizer=tokenizer,\n",
    "\tis_training=True,\n",
    ")\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train-bart-LM.ipynb\"\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "\tproject=\"Test\",\n",
    "\tname=hyperparameters.bart_model_name\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\tmax_epochs=hyperparameters.train_epochs,\n",
    "\taccelerator=\"gpu\",\n",
    "\tlogger=wandb_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    }
   ],
   "source": [
    "class Experiment:\n",
    "\tdef __init__(self,\n",
    "\t\tmodel=None,\n",
    "\t\tdata_module=None,\n",
    "\t):\n",
    "\t\tself.model = model\n",
    "\t\t\n",
    "\t\tself.data_module = data_module\n",
    "\t\ttrain_dataloader = self.data_module.train_dataloader()\n",
    "\t\tvalid_dataloader = self.data_module.val_dataloader()\n",
    "\t\t\n",
    "\t\tself.train_dataloader = train_dataloader\n",
    "\t\tself.valid_dataloader = valid_dataloader\n",
    "\n",
    "\t\tself.optimizer = torch.optim.AdamW(\n",
    "\t\t\tparams=self.model.parameters(),\n",
    "\t\t)\n",
    "\t\n",
    "\tdef train(self):\n",
    "\t\tself.model.train()\n",
    "\t\ttotal_loss = 0\n",
    "\t\tfor batch in self.train_dataloader:\n",
    "\t\t\tinput_ids = batch['input_ids']\n",
    "\t\t\tattention_mask = batch['attention_mask']\n",
    "\t\t\tlabels = input_ids.clone()\n",
    "\n",
    "\t\t\toutputs = self.model.forward(\n",
    "\t\t\t\tinput_ids=input_ids,\n",
    "\t\t\t\tattention_mask=attention_mask,\n",
    "\t\t\t\tlabels=labels,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\ttotal_loss += float(loss.item())\n",
    "\t\t\tprint(loss)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tself.optimizer.step()\n",
    "\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\n",
    "\t\tprint(\"Total loss: \", total_loss / len(self.train_dataloader))\n",
    "\t\n",
    "\tdef valid(self):\n",
    "\t\tself.model.eval()\n",
    "\t\ttotal_valid_loss = 0\n",
    "\t\tfor batch in self.valid_dataloader:\n",
    "\t\t\tinput_ids = batch['input_ids']\n",
    "\t\t\tattention_mask = batch['attention_mask']\n",
    "\t\t\tlabels = input_ids.clone()\n",
    "\n",
    "\t\t\toutputs = self.model.forward(\n",
    "\t\t\t\tinput_ids=input_ids,\n",
    "\t\t\t\tattention_mask=attention_mask,\n",
    "\t\t\t\tlabels=labels,\n",
    "\t\t\t)\n",
    "\t\t\tloss = outputs.loss\n",
    "\t\t\ttotal_valid_loss += float(loss.item())\n",
    "\t\t\tprint(loss)\n",
    "\t\t\n",
    "\t\tprint(\"Total valid loss: \", total_valid_loss / len(self.valid_dataloader))\n",
    "\t\n",
    "\tdef run_experiment(self, epochs=1):\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\tprint(\"Epoch: \", epoch)\n",
    "\t\t\tself.train()\n",
    "\t\t\tself.valid()\n",
    "\n",
    "hyperparameters = BartFoCusDatasetSampleHyperparametersV1()\n",
    "tokenizer = BartFoCusTokenizerV1.from_pretrained(\n",
    "\thyperparameters.bart_model_name, \n",
    "\thyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "model = BartLMV1(\n",
    "\t\t\tconfig=BartConfig.from_pretrained(hyperparameters.bart_model_name),\n",
    "\t\t\thyperparameters=hyperparameters,\n",
    "\t\t\ttokenizer=tokenizer,\n",
    "\t\t)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "data_module = FoCusDataModuleV1(\n",
    "\ttrain_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "\tvalid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "\thyperparameters=hyperparameters,\n",
    "\ttokenizer=tokenizer,\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "experiment = Experiment(\n",
    "\tmodel=model,\n",
    "\tdata_module=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "tensor(11.0113, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.7379, grad_fn=<NllLossBackward0>)\n",
      "tensor(9.0289, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.8907, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.1373, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.1703, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0072, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8486, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5476, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7590, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8910, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7827, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8741, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7762, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.9560, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.9028, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8086, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8627, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2926, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6142, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7929, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6892, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2505, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.0756, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4422, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8535, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7304, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5946, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8131, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6718, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6356, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4707, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.8894, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5390, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4649, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4294, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5614, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3702, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7843, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4194, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4354, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5777, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4093, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4595, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5226, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2734, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7622, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.1752, grad_fn=<NllLossBackward0>)\n",
      "tensor(8.2087, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6055, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5667, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5933, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0730, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4217, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5725, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6445, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7231, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6658, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4423, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4137, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7158, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7347, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4195, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3271, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7478, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4744, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4221, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4894, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.8243, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6536, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2922, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5475, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3109, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3046, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5337, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.9914, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6835, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2828, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2366, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4579, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3143, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.7893, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0699, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4694, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5445, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4691, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4098, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4640, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4376, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6100, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4431, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5543, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3377, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4050, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2689, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.6576, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3664, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3899, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3593, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1823, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4724, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1815, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3436, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4401, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.3844, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2251, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.2142, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3775 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4501, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.5672, grad_fn=<NllLossBackward0>)\n",
      "tensor(7.4441, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 64\u001b[0m, in \u001b[0;36mExperiment.run_experiment\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     63\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m---> 64\u001b[0m \t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid()\n",
      "Cell \u001b[0;32mIn [13], line 36\u001b[0m, in \u001b[0;36mExperiment.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# experiment.run_experiment(epochs=1)\n",
    "\"\"\"\n",
    "tensor(11.0113, grad_fn=<NllLossBackward0>)\n",
    "tensor(9.7379, grad_fn=<NllLossBackward0>)\n",
    "tensor(9.0289, grad_fn=<NllLossBackward0>)\n",
    "tensor(8.8907, grad_fn=<NllLossBackward0>)\n",
    "tensor(8.1373, grad_fn=<NllLossBackward0>)\n",
    "tensor(8.1703, grad_fn=<NllLossBackward0>)\n",
    "tensor(8.0072, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.8486, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.5476, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.7590, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.8910, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.7827, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.8741, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.7762, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.9560, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.9028, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.8086, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.8627, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.2926, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.6142, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.7929, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.6892, grad_fn=<NllLossBackward0>)\n",
    "tensor(7.2505, grad_fn=<NllLossBackward0>)\n",
    "tensor(8.0756, grad_fn=<NllLossBackward0>)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
