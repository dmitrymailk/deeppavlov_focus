{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /home/dimweb/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /home/dimweb/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/dimweb/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 5365, 447, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import StableDropout, DebertaV2Model, ContextPooler\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from typing import Optional, List, Dict, Any, Union, Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DebertaV3ForClassification(DebertaV2ForSequenceClassification):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        num_labels = getattr(config, \"num_labels\", 2)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.pooler = ContextPooler(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim, num_labels)\n",
    "        drop_out = getattr(config, \"cls_dropout\", None)\n",
    "        drop_out = self.config.hidden_dropout_prob if drop_out is None else drop_out\n",
    "        self.dropout = StableDropout(drop_out)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.deberta(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = self.pooler(encoder_layer)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = loss_fn(logits, labels.view(-1))\n",
    "            \n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states, \n",
    "            attentions=outputs.attentions\n",
    "        )\n",
    "\n",
    "# model = DebertaV3ForClassification.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer.encode(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from core.hyperparameters.debertav3_hyperparameters import DebertaV3HyperparametersV1\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from core.dataloaders.focus.lighting.debertav3_lighting_dataloaders import DebertaV3FoCusLightningDataModuleV1, DebertaV3FoCusLightningDataModuleV2 \n",
    "\n",
    "hyperparameters = DebertaV3HyperparametersV1(\n",
    "        train_batch_size=16,\n",
    "        valid_batch_size=16,\n",
    "    )\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\n",
    "        hyperparameters.model_name,\n",
    "    )\n",
    "\n",
    "data_module = DebertaV3FoCusLightningDataModuleV2(\n",
    "        train_path_dataset=\"./datasets/FoCus/train_focus.json\",\n",
    "        valid_path_dataset=\"./datasets/FoCus/valid_focus.json\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        tokenizer=tokenizer,  # type: ignore\n",
    "        debug_status=0,\n",
    "    )\n",
    "    \n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   279, 10367,  ...,     0,     0,     0],\n",
       "         [    1,   606,   307,  ...,     0,     0,     0],\n",
       "         [    1, 10420, 16479,  ..., 15713,   302,     2],\n",
       "         ...,\n",
       "         [    1,   279, 23409,  ...,     0,     0,     0],\n",
       "         [    1,   279, 75313,  ...,     0,     0,     0],\n",
       "         [    1,   434,   340,  ...,     0,     0,     0]]),\n",
       " 'labels': tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'unique_ids': ['WL1G6EX8T9JY_dialogue1',\n",
       "  'NZQX869YZZK9_dialogue6',\n",
       "  'NTGAXOBEIRLQ_dialogue6',\n",
       "  'ZANR2M0FJKWF_dialogue1',\n",
       "  'EKEMEF4T3OHH_dialogue5',\n",
       "  'VTFNAHV7OIG4_dialogue5',\n",
       "  'HYOQLKN3XGCD_dialogue5',\n",
       "  'KAN28FJWSTJP_dialogue4',\n",
       "  'P878Z7YI2MMT_dialogue1',\n",
       "  'YJH7UTD0DR6A_dialogue4',\n",
       "  'K74N0MZIW09R_dialogue6',\n",
       "  'Q3EABP9MAEWV_dialogue4',\n",
       "  'DYFMDYGGVUWG_dialogue6',\n",
       "  'L4X67KG6MQ2Z_dialogue1',\n",
       "  'DKQ8U7DLSBYM_dialogue4',\n",
       "  'OEU6SDO0AGQ9_dialogue5']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_module.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
