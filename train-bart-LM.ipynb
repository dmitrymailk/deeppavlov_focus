{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[123, 123], [1, 2]]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str([[123, 123], [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Dict, TypedDict\n",
    "import json\n",
    "from transformers import AutoTokenizer, BartTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "class TF_IDF:\n",
    "\tdef __init__(self, \n",
    "\t\t\tcorpus: List[List[int]],\n",
    "\t\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tExample usage:\n",
    "\t\t\tcorpus = [\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2],\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\ttf_idf = TF_IDF(corpus)\n",
    "\t\t\t\n",
    "\t\t\tsimilar_sentences = tf_idf.get_similar([1, 2, 3], n=3)\n",
    "\t\t\t>>> similar_sentences\n",
    "\t\t\t[\n",
    "\t\t\t\t[1, 2, 3],\n",
    "\t\t\t\t[1, 2, 3, 4],\n",
    "\t\t\t\t[1, 2]\n",
    "\t\t\t]\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tcorpus (List[List[int]]): токенизированный корпус\n",
    "\t\t\"\"\"\n",
    "\t\tself.vectorizer = TfidfVectorizer(\n",
    "\t\t\t# token_pattern is number\n",
    "\t\t\ttoken_pattern=r\"(?u)\\b\\d+\\b\", \n",
    "\t\t)\n",
    "\t\tnew_corpus = self.__encode_sentences(corpus)\n",
    "\n",
    "\t\tself.X = self.vectorizer.fit_transform(new_corpus)\n",
    "\t\tself.corpus = corpus\n",
    "\t\n",
    "\tdef __encode_sentence(self, sentence: List[int]) -> str:\n",
    "\t\treturn \" \".join(list(map(str, sentence)))\n",
    "\n",
    "\tdef __encode_sentences(self, sentences: List[List[int]]) -> List[str]:\n",
    "\t\treturn list(map(self.__encode_sentence, sentences))\n",
    "\t\n",
    "\tdef top_similar(self, \n",
    "\t\t\tquery: List[List[int]] = None,\n",
    "\t\t\ttop_k: int = 1,\n",
    "\t\t) -> List[List[int]]:\n",
    "\t\tquery = self.__encode_sentences(query)\n",
    "\t\tquery = self.vectorizer.transform(query)\n",
    "\t\t\n",
    "\t\tsimilarity = cosine_similarity(self.X, query)\n",
    "\t\tsimilarity = similarity.flatten()\n",
    "\t\tsimilarity = np.argsort(similarity)[::-1][:top_k]\n",
    "\t\tsimilarity = similarity.tolist()\n",
    "\n",
    "\t\tsimilar_samples = [self.corpus[i] for i in similarity]\n",
    "\t\treturn similar_samples\n",
    "\n",
    "class FoCusTF_IDF(TF_IDF):\n",
    "\tdef __init__(self,\n",
    "\t\t**kwargs,\n",
    "\t) -> None:\n",
    "\t\tsuper().__init__(**kwargs)\n",
    "\n",
    "\t\tself.cached_similar = {}\n",
    "\t\n",
    "\tdef top_similar(self, \n",
    "\t\t\tquery: List[List[int]] = None, \n",
    "\t\t\ttop_k: int = 1\n",
    "\t\t) -> List[List[int]]:\n",
    "\t\tquery_str = str(query)\n",
    "\n",
    "\t\tif query_str in self.cached_similar:\n",
    "\t\t\treturn self.cached_similar[query_str]\n",
    "\t\t\n",
    "\t\tsimilar_samples = super().top_similar(\n",
    "\t\t\tquery=query,\n",
    "\t\t\ttop_k=top_k,\n",
    "\t\t)\n",
    "\t\tself.cached_similar[query_str] = similar_samples\n",
    "\n",
    "\t\treturn similar_samples\n",
    "class FoCusDatasetSampleDictV1(TypedDict):\n",
    "\tpersona: List[str]\n",
    "\tknowledge_candidates: List[str]\n",
    "\tpersona_grounding: List[int]\n",
    "\tdialog: List[int]\n",
    "\tknowledge_answer_index: int\n",
    "\tknowledge: List[str]\n",
    "\n",
    "class FoCusDatasetSampleV1:\n",
    "\t__slots__ = (\n",
    "\t\t'persona', \n",
    "\t\t'knowledge_candidates',  \n",
    "\t\t'persona_grounding', \n",
    "\t\t'dialog', \n",
    "\t\t'knowledge_answer_index',\n",
    "\t\t\"knowledge\"\n",
    "\t)\n",
    "\n",
    "\tdef __init__(self, \n",
    "\t\t\tpersona: List[str],\n",
    "\t\t\tknowledge_candidates: List[str],\n",
    "\t\t\tpersona_grounding: List[int],\n",
    "\t\t\tdialog: List[str],\n",
    "\t\t\tknowledge: List[str],\n",
    "\t\t\tknowledge_answer_index: int,\n",
    "\t\t) -> None:\n",
    "\t\tself.persona = persona\n",
    "\t\tself.knowledge_candidates = knowledge_candidates\n",
    "\t\tself.persona_grounding = persona_grounding\n",
    "\t\tself.knowledge_answer_index = knowledge_answer_index\n",
    "\t\tself.dialog = dialog\n",
    "\t\tself.knowledge = knowledge\n",
    "\t\n",
    "\tdef get_dict(self) -> FoCusDatasetSampleDictV1:\n",
    "\t\treturn {\n",
    "\t\t\t'persona': self.persona,\n",
    "\t\t\t'knowledge_candidates': self.knowledge_candidates,\n",
    "\t\t\t'persona_grounding': self.persona_grounding,\n",
    "\t\t\t'dialog': self.dialog,\n",
    "\t\t\t'knowledge_answer_index': self.knowledge_answer_index,\n",
    "\t\t\t'knowledge': self.knowledge,\n",
    "\t\t}\n",
    "\n",
    "class BartFoCusDatasetSampleHyperparametersV1:\n",
    "\tdef __init__(self,\n",
    "\t\t\tdialog_history_length: int = 1,\n",
    "\t\t\tcontext_length: int = 1,\n",
    "\t\t\tknowledge_length: int = 1,\n",
    "\t\t) -> None:\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdialog_history_length (int): количество пар диалогов(назад), которые будут \n",
    "\t\t\t\tиспользоваться для генерации ответа\t\n",
    "\t\t\tcontext_length (int): количество предложений из диалога, относительно которых \n",
    "\t\t\t\tбудут выбираться похожие из поля knowledge\n",
    "\t\t\tknowledge_length (int): количество предложений из knowledge, которые будут\n",
    "\t\t\t\tподаваться на вход модели \n",
    "\t\t\"\"\"\n",
    "\t\tself.dialog_history_length = 1\n",
    "\t\tself.context_length = 1\n",
    "\t\tself.knowledge_length = 1\n",
    "\t\t\n",
    "\t\tself.max_persona_tokens = 200\n",
    "\t\tself.max_dialog_history_tokens = 200\n",
    "\t\tself.max_knowledge_tokens = 200\n",
    "\t\tself.max_bot_response_tokens = 150\n",
    "\n",
    "\t\tself.dialog_bos_token = '<dialog>'\n",
    "\t\tself.dialog_eos_token = '</dialog>'\n",
    "\n",
    "class BartFoCusTokenizerV1(BartTokenizer):\n",
    "\tdef __init__(self,\n",
    "\t\t\t*args,\n",
    "\t\t\t**kwargs \n",
    "\t\t) -> None:\n",
    "\t\tsuper().__init__(**kwargs)\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_pretrained(cls, \n",
    "\t\t\t*args, \n",
    "\t\t\thyperparameters: BartFoCusDatasetSampleHyperparametersV1 = None, \n",
    "\t\t\t**kwargs\n",
    "\t\t):\n",
    "\t\t\n",
    "\t\ttokenizer: BartTokenizer = BartTokenizer.from_pretrained(*args, **kwargs)\n",
    "\t\t\n",
    "\t\tif hyperparameters is not None:\n",
    "\t\t\ttokens = [\n",
    "\t\t\t\thyperparameters.dialog_bos_token,\n",
    "\t\t\t\thyperparameters.dialog_eos_token,\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\ttokenizer.add_special_tokens({'additional_special_tokens': tokens})\n",
    "\n",
    "\t\treturn tokenizer\n",
    "\n",
    "class BartFoCusDatasetSampleDictV1(TypedDict):\n",
    "\tinput_ids: List[int]\n",
    "\tattention_mask: List[int]\n",
    "\n",
    "class BartFoCusDatasetSampleV1:\n",
    "\t\"\"\"\n",
    "\t[BOS][persona][SEP][knowledge][SEP][dialog][:-1][SEP]<dialog>[dialog][-1]</dialog> \n",
    "\t- [dialog] - набор диалоговых пар\n",
    "\t- persona - все предложения персоны\n",
    "\t- knowledge - топ наиболее похожих предложений из knowledge к контексту диалога\n",
    "\t- [dialog][:-1] - все диалоговые пары, кроме ответа бота\n",
    "\t- <dialog>[dialog][-1]</dialog> - ответ бота \n",
    "\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, \n",
    "\t\t\tfocus_dataset_sample: FoCusDatasetSampleDictV1,\n",
    "\t\t\ttokenizer: BartFoCusTokenizerV1,\n",
    "\t\t\th_params: BartFoCusDatasetSampleHyperparametersV1,\n",
    "\t\t) -> None:\n",
    "\t\tself.focus_dataset_sample = focus_dataset_sample\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.h_params = h_params\n",
    "\n",
    "\t\tself.bos_token_id = self.tokenizer.bos_token_id\n",
    "\t\tself.pad_token_id = self.tokenizer.pad_token_id\n",
    "\t\tself.unk_token_id = self.tokenizer.unk_token_id\n",
    "\t\tself.sep_token_id = self.tokenizer.sep_token_id\n",
    "\t\tself.cls_token_id = self.tokenizer.cls_token_id\n",
    "\n",
    "\t\tself.dialog_bos = self.__get_token_id(h_params.dialog_bos_token)\n",
    "\t\tself.dialog_eos = self.__get_token_id(h_params.dialog_eos_token)\n",
    "\t\n",
    "\tdef __get_token_id(self, token: str) -> int:\n",
    "\t\treturn self.tokenizer.convert_tokens_to_ids(token)\n",
    "\t\n",
    "\tdef __flat_list(self, list_of_lists: List[List]) -> List:\n",
    "\t\treturn list(chain.from_iterable(list_of_lists))\n",
    "\n",
    "\tdef get_dict(self) -> BartFoCusDatasetSampleDictV1:\n",
    "\t\tdialog_history_length = self.h_params.dialog_history_length\n",
    "\t\tcontext_length = self.h_params.context_length\n",
    "\t\tknowledge_length = self.h_params.knowledge_length\n",
    "\n",
    "\t\tpersona = self.focus_dataset_sample['persona']\n",
    "\t\tdialog = self.focus_dataset_sample['dialog']\n",
    "\t\tknowledge = self.focus_dataset_sample['knowledge']\n",
    "\n",
    "\t\tencoded_persona = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tpersona, \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\n",
    "\t\tdialog_history = dialog[-2*dialog_history_length:]\n",
    "\t\tdialog_history_feature = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tdialog_history[:-1], \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\t\tdialog_history_target = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tdialog_history[-1:], \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\n",
    "\t\t# контекст на основе которого подбирается knowledge\n",
    "\t\tquery_context = dialog_history_feature['input_ids'][-context_length:]\n",
    "\t\tencoded_knowledge = self.tokenizer.batch_encode_plus(\n",
    "\t\t\tknowledge, \n",
    "\t\t\tadd_special_tokens=False\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\ttf_idf = FoCusTF_IDF(corpus=encoded_knowledge['input_ids'])\n",
    "\t\tmost_similar_knowledge = tf_idf.top_similar(\n",
    "\t\t\tquery=query_context,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# [BOS][persona][SEP][knowledge][SEP][dialog][:-1][SEP]<dialog>[dialog][-1]</dialog>\n",
    "\t\tflat_persona = self.__flat_list(encoded_persona['input_ids'])\n",
    "\t\tflat_knowledge = self.__flat_list(most_similar_knowledge)\n",
    "\t\tflat_dialog_history = self.__flat_list(dialog_history_feature['input_ids'])\n",
    "\t\tflat_bot_response = self.__flat_list(dialog_history_target['input_ids'])\n",
    "\n",
    "\t\tflat_persona = flat_persona[:self.h_params.max_persona_tokens]\n",
    "\t\tflat_knowledge = flat_knowledge[:self.h_params.max_knowledge_tokens]\n",
    "\t\tflat_dialog_history = flat_dialog_history[:self.h_params.max_dialog_history_tokens]\n",
    "\t\tflat_bot_response = flat_bot_response[:self.h_params.max_bot_response_tokens]\n",
    "\n",
    "\t\tinput_sequence = [\n",
    "\t\t\tself.bos_token_id,\n",
    "\t\t\t*flat_persona,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_knowledge,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\t*flat_dialog_history,\n",
    "\t\t\tself.sep_token_id,\n",
    "\t\t\tself.dialog_bos,\n",
    "\t\t\t*flat_bot_response,\n",
    "\t\t\tself.dialog_eos\n",
    "\t\t]\n",
    "\n",
    "\t\tattention_mask = [1] * len(input_sequence)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids': input_sequence,\n",
    "\t\t\t'attention_mask': attention_mask,\n",
    "\t\t}\n",
    "\t\t\n",
    "\n",
    "class FoCusDatasetV1:\n",
    "\tdef __init__(self,\n",
    "\t\tinput_dataset_path: str = None,\n",
    "\t\t) -> None:\n",
    "\t\tassert input_dataset_path is not None, 'input_dataset_path is None'\n",
    "\n",
    "\t\tself.input_dataset_path: str = input_dataset_path\n",
    "\t\tself.dataset: List[FoCusDatasetSampleDictV1] = []\n",
    "\n",
    "\t\tself.__build_dataset()\n",
    "\t\n",
    "\tdef __build_dataset(self) -> None:\n",
    "\t\tinitial_dataset = self.__read_dataset(self.input_dataset_path)\n",
    "\t\tself.dataset = self.__create_initial_dataset(initial_dataset=initial_dataset)\n",
    "\t\n",
    "\tdef __create_initial_dataset(self, initial_dataset: Dict = None) -> List[FoCusDatasetSampleDictV1]:\n",
    "\t\tdataset = []\n",
    "\t\tinitial_dataset_data = initial_dataset['data']\n",
    "\t\t\n",
    "\t\tfor i, dialog_set in enumerate(initial_dataset_data):\n",
    "\t\t\tpersona = dialog_set['persona']\n",
    "\t\t\tutterances = dialog_set['utterance']\n",
    "\t\t\tknowledge = dialog_set['knowledge']\n",
    "\t\t\t\n",
    "\t\t\tfor j, utterance in enumerate(utterances):\n",
    "\t\t\t\tpersona_grounding = list(map(int, utterance['persona_grounding']))\n",
    "\t\t\t\tknowledge_candidates = utterance['knowledge_candidates']\n",
    "\t\t\t\tknowledge_answer_index = utterance['knowledge_answer_index']\n",
    "\t\t\t\tdialog_index_key = [item for item in utterance.keys() if 'dialog' in item][0]\n",
    "\t\t\t\tdialog = utterance[dialog_index_key]\n",
    "\t\t\t\t\n",
    "\t\t\t\tdata_sample = FoCusDatasetSampleV1(\n",
    "\t\t\t\t\tpersona=persona,\n",
    "\t\t\t\t\tknowledge_candidates=knowledge_candidates,\n",
    "\t\t\t\t\tpersona_grounding=persona_grounding,\n",
    "\t\t\t\t\tdialog=dialog,\n",
    "\t\t\t\t\tknowledge_answer_index=knowledge_answer_index,\n",
    "\t\t\t\t\tknowledge=knowledge,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tdata_sample = data_sample.get_dict()\n",
    "\t\t\t\tdataset.append(data_sample)\n",
    "\t\t\n",
    "\t\treturn dataset\n",
    "\t\n",
    "\tdef __read_dataset(self, input_path: str) -> list:\n",
    "\t\twith open(input_path, 'r') as f:\n",
    "\t\t\tdataset = json.load(f)\n",
    "\t\treturn dataset\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleDictV1:\n",
    "\t\treturn self.dataset[index]\n",
    "\n",
    "class PytorchFoCusDatasetV1(Dataset):\n",
    "\tdef __init__(self, \n",
    "\t\t\tdataset: FoCusDatasetV1,\n",
    "\t\t) -> None:\n",
    "\t\tself.dataset = dataset\n",
    "\t\tself.bart_hyperparameters = BartFoCusDatasetSampleHyperparametersV1()\n",
    "\t\tself.bart_tokenizer = BartFoCusTokenizerV1.from_pretrained(\n",
    "\t\t\t'facebook/bart-large',\n",
    "\t\t\thyperparameters=self.bart_hyperparameters\n",
    "\t\t)\n",
    "\t\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.dataset)\n",
    "\t\n",
    "\tdef __getitem__(self, index: int) -> FoCusDatasetSampleDictV1:\n",
    "\t\tdataset_sample = self.dataset[index]\n",
    "\t\ttrain_sample = BartFoCusDatasetSampleV1(\n",
    "\t\t\tfocus_dataset_sample=dataset_sample,\n",
    "\t\t\ttokenizer=self.bart_tokenizer,\n",
    "\t\t\th_params=self.bart_hyperparameters,\n",
    "\t\t)\n",
    "\t\treturn train_sample\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
