{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "# model_name = \"microsoft/deberta-v3-base\"\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataloaders.focus.focus_dataloader import FoCusDatasetPersonaV2\n",
    "from core.utils import PytorchDatasetFactory\n",
    "from core.dataloaders.focus.models.debertav3_dataloaders import DebertaV3FoCusPersonaDatasetSampleV2\n",
    "from core.hyperparameters.debertav3_hyperparameters import DebertaV3HyperparametersV1\n",
    "\n",
    "hyperparameters = DebertaV3HyperparametersV1(\n",
    "    train_batch_size=16,\n",
    "    valid_batch_size=16,\n",
    "    max_dialog_history_tokens=70,\n",
    "    max_knowledge_candidates_tokens=220,\n",
    "    max_persona_tokens=20,\n",
    "    model_name=model_name,\n",
    "    project_name=\"focus_persona_classification\",\n",
    ")\n",
    "\n",
    "train_dataset = FoCusDatasetPersonaV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/train_focus.json\",\n",
    "    is_train=True,\n",
    ")\n",
    "\n",
    "\n",
    "valid_dataset = FoCusDatasetPersonaV2(\n",
    "    input_dataset_path=\"./datasets/FoCus/valid_focus.json\",\n",
    "    is_train=False,\n",
    ")\n",
    "\n",
    "train_dataset = PytorchDatasetFactory(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    hyperparameters=hyperparameters,\n",
    "    dataset_sample_class=DebertaV3FoCusPersonaDatasetSampleV2,\n",
    ")\n",
    "\n",
    "valid_dataset = PytorchDatasetFactory(\n",
    "    dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    hyperparameters=hyperparameters,\n",
    "    dataset_sample_class=DebertaV3FoCusPersonaDatasetSampleV2,\n",
    ")\n",
    "\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=focus_persona_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 114630\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57316\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdimweb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dimweb/Desktop/deeppavlov/my_focus/wandb/run-20221010_115723-2mclixop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dimweb/focus_persona_classification/runs/2mclixop\" target=\"_blank\">huggingface_microsoft/deberta-v3-small</a></strong> to <a href=\"https://wandb.ai/dimweb/focus_persona_classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10395' max='57316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10395/57316 22:25 < 1:41:14, 7.72 it/s, Epoch 0.73/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.550953</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.689300</td>\n",
       "      <td>0.642732</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.665300</td>\n",
       "      <td>0.579188</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.647100</td>\n",
       "      <td>0.541777</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.683800</td>\n",
       "      <td>0.524018</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.521085</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.680800</td>\n",
       "      <td>0.538823</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.664600</td>\n",
       "      <td>0.550075</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.647300</td>\n",
       "      <td>0.518460</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.667200</td>\n",
       "      <td>0.595386</td>\n",
       "      <td>0.866998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-1000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-1500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-2000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-2500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-3000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-3500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-3500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-4000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-4500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-4500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-5000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-5500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-5500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-5500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-6000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-6500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-6500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-6500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-7000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-7000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-7500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-7500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-7500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-8000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-8000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-8500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-8500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-8500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-9000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-9000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-9500\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-9500/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-9500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 28195\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: unique_id. If unique_id are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./results/microsoft/deberta-v3-small/checkpoint-10000\n",
      "Configuration saved in ./results/microsoft/deberta-v3-small/checkpoint-10000/config.json\n",
      "Model weights saved in ./results/microsoft/deberta-v3-small/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/microsoft/deberta-v3-small/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/microsoft/deberta-v3-small/checkpoint-10000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 29\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results/\u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     do_train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1503\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1743\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1744\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1745\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1746\u001b[0m ):\n\u001b[1;32m   1747\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/trainer.py:2470\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2469\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2470\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2473\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/trainer.py:2502\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2501\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2502\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2503\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1282\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1282\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   1283\u001b[0m     input_ids,\n\u001b[1;32m   1284\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1285\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1286\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1287\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1288\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1289\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1290\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1294\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1053\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1045\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1046\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1047\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1051\u001b[0m )\n\u001b[0;32m-> 1053\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1054\u001b[0m     embedding_output,\n\u001b[1;32m   1055\u001b[0m     attention_mask,\n\u001b[1;32m   1056\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1057\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1058\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1059\u001b[0m )\n\u001b[1;32m   1060\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1062\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:502\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    493\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    494\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    495\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m         rel_embeddings,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    503\u001b[0m         next_kv,\n\u001b[1;32m    504\u001b[0m         attention_mask,\n\u001b[1;32m    505\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    506\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    507\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    508\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    509\u001b[0m     )\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    512\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:346\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    338\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    339\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    345\u001b[0m ):\n\u001b[0;32m--> 346\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    347\u001b[0m         hidden_states,\n\u001b[1;32m    348\u001b[0m         attention_mask,\n\u001b[1;32m    349\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    350\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    351\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    352\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    355\u001b[0m         attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:277\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ):\n\u001b[0;32m--> 277\u001b[0m     self_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    278\u001b[0m         hidden_states,\n\u001b[1;32m    279\u001b[0m         attention_mask,\n\u001b[1;32m    280\u001b[0m         output_attentions,\n\u001b[1;32m    281\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    282\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    283\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    286\u001b[0m         self_output, att_matrix \u001b[39m=\u001b[39m self_output\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:702\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    701\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> 702\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    703\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    704\u001b[0m     )\n\u001b[1;32m    706\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m~/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:746\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRelative position ids must be of dim 2 or 3 or 4. \u001b[39m\u001b[39m{\u001b[39;00mrelative_pos\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    745\u001b[0m att_span \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_ebd_size\n\u001b[0;32m--> 746\u001b[0m relative_pos \u001b[39m=\u001b[39m relative_pos\u001b[39m.\u001b[39;49mlong()\u001b[39m.\u001b[39;49mto(query_layer\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    748\u001b[0m rel_embeddings \u001b[39m=\u001b[39m rel_embeddings[\u001b[39m0\u001b[39m : att_span \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, :]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_att_key:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=focus_persona_classification\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{model_name}\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.02,\n",
    "    logging_steps=10,\n",
    "    overwrite_output_dir=True,\n",
    "    run_name=f\"huggingface_{model_name}\",\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    do_train=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('d_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c47eeeae5f0593d6ff7164e36f6d45daaa118b41372aa3e9757d1f066e1c76d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
